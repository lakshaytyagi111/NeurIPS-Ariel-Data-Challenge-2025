{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":101849,"databundleVersionId":13093295,"sourceType":"competition"},{"sourceId":13660570,"sourceType":"datasetVersion","datasetId":8685254},{"sourceId":13660978,"sourceType":"datasetVersion","datasetId":8685519}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the first working notebook pipeline for the areal challenge solution","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport os\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nprint(\"Imported Libraries Successfully\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:53:09.399991Z","iopub.execute_input":"2025-11-14T16:53:09.400518Z","iopub.status.idle":"2025-11-14T16:53:10.317842Z","shell.execute_reply.started":"2025-11-14T16:53:09.400496Z","shell.execute_reply":"2025-11-14T16:53:10.317053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VERSION = \"v3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:53:14.370929Z","iopub.execute_input":"2025-11-14T16:53:14.371338Z","iopub.status.idle":"2025-11-14T16:53:14.374973Z","shell.execute_reply.started":"2025-11-14T16:53:14.371316Z","shell.execute_reply":"2025-11-14T16:53:14.374310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# !ls -lh /kaggle/working\n\n# import shutil\n\n# # Compress your .npy file\n# shutil.make_archive('/kaggle/working/signal_v2', 'zip', '/kaggle/working', 'signal_v2.npy')\n\n# from IPython.display import FileLink\n# FileLink('/kaggle/input/ariel-data-challenge-2025/train/1024292144/AIRS-CH0_signal_0.parquet')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:05:48.562907Z","iopub.execute_input":"2025-11-10T20:05:48.563203Z","iopub.status.idle":"2025-11-10T20:05:48.583646Z","shell.execute_reply.started":"2025-11-10T20:05:48.563175Z","shell.execute_reply":"2025-11-10T20:05:48.582706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile preprocess.py\n\nimport pandas as pd\nimport numpy as np\nimport multiprocessing as mp\nimport itertools\nimport os\nimport subprocess\nfrom astropy.stats import sigma_clip\nfrom numpy.polynomial import Polynomial\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\n\n\nROOT = \"/kaggle/input/ariel-data-challenge-2025/\"\nVERSION = \"v2\"\nA_BINNING = 15\nF_BINNING = 12*15\n\n\ndevice = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device : \", device)\nMODE = os.getenv('PREPROCESS_MODE')\n\n\nsensor_sizes_dict = {\n    \"AIRS-CH0\": [[11250, 32, 356], [32, 356]],\n    \"FGS1\": [[135000, 32, 32], [32, 32]],\n}  # input, mask\n\ncl = 8\ncr = 24\n\n\ndef get_gain_offset():\n    \"\"\"\n    Get the gain and offset for a given planet and sensor\n\n    Unlike last year's challenge, all planets use the same adc_info.\n    We can just hard code it.\n    \"\"\"\n    gain = 0.4369\n    offset = -1000.0\n    return gain, offset\n\n\n\ndef read_data(planet_id, sensor, mode):\n    \"\"\"\n    Read the data for a given planet and sensor\n    \"\"\"\n    # get all noise correction frames and signal\n    signal = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_signal_0.parquet\",\n        engine=\"pyarrow\",\n    )\n    dark_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/dark.parquet\",\n        engine=\"pyarrow\",\n    )\n    dead_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/dead.parquet\",\n        engine=\"pyarrow\",\n    )\n    linear_corr_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/linear_corr.parquet\",\n        engine=\"pyarrow\",\n    )\n    flat_frame = pd.read_parquet(\n        f\"{ROOT}/{mode}/{planet_id}/{sensor}_calibration_0/flat.parquet\",\n        engine=\"pyarrow\",\n    )\n\n    # reshape to sensor shape and cast to float64\n    signal = signal.values.astype(np.float64).reshape(sensor_sizes_dict[sensor][0])[\n        :, cl:cr, :\n    ]\n    dark_frame = dark_frame.values.astype(np.float64).reshape(\n        sensor_sizes_dict[sensor][1]\n    )[cl:cr, :]\n    dead_frame = dead_frame.values.reshape(sensor_sizes_dict[sensor][1])[cl:cr, :]\n    flat_frame = flat_frame.values.astype(np.float64).reshape(\n        sensor_sizes_dict[sensor][1]\n    )[cl:cr, :]\n\n    linear_corr = linear_corr_frame.values.astype(np.float64).reshape(\n        [6] + sensor_sizes_dict[sensor][1]\n    )[:, cl:cr, :]\n\n    return (\n        signal,\n        dark_frame,\n        dead_frame,\n        linear_corr,\n        flat_frame,\n    )\n\n\ndef ADC_convert(signal, gain, offset):\n    \"\"\"\n    Step 1: Analog-to-Digital Conversion (ADC) correction\n\n    The Analog-to-Digital Conversion (adc) is performed by the detector to convert the\n    pixel voltage into an integer number. We revert this operation by using the gain\n    and offset for the calibration files 'train_adc_info.csv'.\n    \"\"\"\n\n    signal /= gain\n    signal += offset\n    return signal\n\n\ndef mask_hot_dead(signal, dead, dark):\n    \"\"\"\n    Step 2: Mask hot/dead pixel\n\n    The dead pixels map is a map of the pixels that do not respond to light and, thus,\n    can't be accounted for any calculation. In all these frames the dead pixels are\n    masked using python masked arrays. The bad pixels are thus masked but left\n    uncorrected. Some methods can be used to correct bad-pixels but this task,\n    if needed, is left to the participants.\n    \"\"\"\n\n    hot = sigma_clip(dark, sigma=5, maxiters=5).mask\n    hot = np.tile(hot, (signal.shape[0], 1, 1))\n    dead = np.tile(dead, (signal.shape[0], 1, 1))\n\n    signal[dead] = np.nan\n    signal[hot] = np.nan\n    return signal\n\n\ndef apply_linear_corr(c, signal):\n    \"\"\"\n    Step 3: linearity Correction\n\n    The non-linearity of the pixels' response can be explained as capacitive leakage\n    on the readout electronics of each pixel during the integration time. The number\n    of electrons in the well is proportional to the number of photons that hit the\n    pixel, with a quantum efficiency coefficient. However, the response of the pixel\n    is not linear with the number of electrons in the well. This effect can be\n    described by a polynomial function of the number of electrons actually in the well.\n    The data is provided with calibration files linear_corr.parquet that are the\n    coefficients of the inverse polynomial function and can be used to correct this\n    non-linearity effect.\n    Using horner's method to evaluate the polynomial\n    \"\"\"\n    assert c.shape[0] == 6  # Ensure the polynomial is of degree 5\n\n    return (\n        (((c[5] * signal + c[4]) * signal + c[3]) * signal + c[2]) * signal + c[1]\n    ) * signal + c[0]\n\n\ndef clean_dark(signal, dark, dt):\n    \"\"\"\n    Step 4: dark current subtraction\n\n    The data provided include calibration for dark current estimation, which can be\n    used to pre-process the observations. Dark current represents a constant signal\n    that accumulates in each pixel during the integration time, independent of the\n    incoming light. To obtain the corrected image, the following conventional approach\n    is applied: The data provided include calibration files such as dark frames or\n    dead pixels' maps. They can be used to pre-process the observations. The dark frame\n    is a map of the detector response to a very short exposure time, to correct for the\n    dark current of the detector.\n\n    image - (dark * dt)\n\n    The corrected image is conventionally obtained via the following: where the dark\n    current map is first corrected for the dead pixel.\n    \"\"\"\n\n    dark = torch.tile(dark, (signal.shape[0], 1, 1))\n    signal -= dark * dt[:, None, None]\n    return signal\n\n\ndef get_cds(signal):\n    \"\"\"\n    Step 5: Get Correlated Double Sampling (CDS)\n\n    The science frames are alternating between the start of the exposure and the end of\n    the exposure. The lecture scheme is a ramp with a double sampling, called\n    Correlated Double Sampling (CDS), the detector is read twice, once at the start\n    of the exposure and once at the end of the exposure. The final CDS is the\n    difference (End of exposure) - (Start of exposure).\n    \"\"\"\n\n    return torch.subtract(signal[1::2, :, :], signal[::2, :, :])\n\n\ndef correct_flat_field(flat, signal):\n    \"\"\"\n    Step 6: Flat Field Correction\n\n    The flat field is a map of the detector response to uniform illumination, to\n    correct for the pixel-to-pixel variations of the detector, for example the\n    different quantum efficiencies of each pixel.\n    \"\"\"\n\n    return signal / flat\n\n\ndef bin_obs(signal, binning):\n    \"\"\"\n    Step 5.1: Bin Observations\n\n    The data provided are binned in the time dimension. The binning is performed by\n    summing the signal over the time dimension.\n    \"\"\"\n\n    cds_binned = torch.zeros(\n        (\n            signal.shape[0] // binning,\n            signal.shape[1],\n            signal.shape[2],\n        ),\n        device=device,\n    )\n    for i in range(signal.shape[0] // binning):\n        cds_binned[i, :, :] = torch.sum(\n            signal[i * binning : (i + 1) * binning, :, :], axis=0\n        )\n    return cds_binned\n\n\ndef nan_interpolation(tensor):\n    # Assume tensor is of shape (batch, height, width)\n    nan_mask = torch.isnan(tensor)\n\n    # Replace NaNs with zero temporarily\n    tensor_filled = torch.where(\n        nan_mask, torch.tensor(0.0, device=tensor.device), tensor\n    )\n\n    # Create a binary mask (0 where NaNs were and 1 elsewhere)\n    ones = torch.ones_like(tensor, device=tensor.device)\n    weight = torch.where(nan_mask, torch.tensor(0.0, device=tensor.device), ones)\n\n    # Perform interpolation by convolving with a kernel\n    # using bilinear interpolation\n    kernel = torch.ones(1, 1, 1, 3, device=tensor.device, dtype=tensor.dtype)\n\n    # Apply padding to the tensor and weight to prevent boundary issues\n    tensor_padded = F.pad(\n        tensor_filled.unsqueeze(1), (1, 1, 0, 0), mode=\"replicate\"\n    ).squeeze(1)\n    weight_padded = F.pad(weight.unsqueeze(1), (1, 1, 0, 0), mode=\"replicate\").squeeze(\n        1\n    )\n\n    # Convolve the filled tensor and the weight mask\n    tensor_conv = F.conv2d(tensor_padded.unsqueeze(1), kernel, stride=1)\n    weight_conv = F.conv2d(weight_padded.unsqueeze(1), kernel, stride=1)\n\n    # Compute interpolated values (normalized by weights)\n    interpolated_tensor = tensor_conv / weight_conv\n\n    # Apply the interpolated values only to the positions of NaNs\n    result = torch.where(nan_mask, interpolated_tensor.squeeze(1), tensor)\n\n    return result\n\n\n\ndef process_planet(planet_id):\n    \"\"\"\n    Process a single planet's data\n    \"\"\"\n    axis_info = pd.read_parquet(ROOT + \"/axis_info.parquet\")\n    dt_airs = axis_info[\"AIRS-CH0-integration_time\"].dropna().values\n\n    for sensor in [\"AIRS-CH0\", \"FGS1\"]:\n        # load all data for this planet and sensor\n        signal, dark_frame, dead_frame, linear_corr, flat_frame = read_data(\n            planet_id, sensor, mode=MODE\n        )\n        gain, offset = get_gain_offset()\n\n        # Step 1: ADC correction\n        signal = ADC_convert(signal, gain, offset)\n\n        # Step 2: Mask hot/dead pixel\n        signal = mask_hot_dead(signal, dead_frame, dark_frame)\n\n        # clip at 0\n        signal = torch.tensor(signal.clip(0)).to(device)    \n        signal = apply_linear_corr(\n            torch.tensor(linear_corr).to(device), signal.clone().detach()\n        )\n\n        if sensor == \"FGS1\":\n            dt = torch.ones(len(signal), device=device) * 0.1\n        elif sensor == \"AIRS-CH0\":\n            dt = torch.tensor(dt_airs).to(device)\n\n        dt[1::2] += 0.1\n\n        signal = clean_dark(signal, torch.tensor(dark_frame).to(device), dt)\n\n        # Step 5: Get Correlated Double Sampling (CDS)\n        signal = get_cds(signal)\n\n        # Step 6: Flat Field Correction\n\n        if sensor == \"AIRS-CH0\":\n            signal = bin_obs(signal, binning=A_BINNING)\n        else:\n            signal = bin_obs(signal, binning=F_BINNING)\n\n        signal = correct_flat_field(torch.tensor(flat_frame).to(device), signal)\n\n        # Step 7: Interpolate NaNs (twice!)\n        signal = nan_interpolation(signal)\n        signal = nan_interpolation(signal)\n\n        if sensor == \"FGS1\":\n            signal = torch.nanmean(signal, axis=[1, 2]).cpu().numpy()\n        elif sensor == \"AIRS-CH0\":\n            signal = torch.nanmean(signal, axis=1).cpu().numpy()\n            \n        # save the processed signal\n        np.save(\n            str(planet_id) + \"_\" + sensor + f\"_signal_{VERSION}.npy\",\n            signal.astype(np.float64),\n        )\n\nif __name__ == \"__main__\":\n    star_info = pd.read_csv(ROOT + f\"/{MODE}_star_info.csv\", index_col=\"planet_id\")\n    planet_ids = [int(x) for x in star_info.index.tolist()]\n\n    # Use up to 4 threads!\n    mp.set_start_method('spawn')\n    with mp.Pool(processes=4) as pool:\n        list(tqdm(pool.imap(process_planet, planet_ids), total=len(planet_ids)))\n\n    \n    signal_train = []\n\n    for planet_id in planet_ids:\n        f_raw = np.load(f\"{planet_id}_FGS1_signal_{VERSION}.npy\")\n        a_raw = np.load(f\"{planet_id}_AIRS-CH0_signal_{VERSION}.npy\")\n\n        # flip a_raw\n        signal = np.concatenate([f_raw[:, None], a_raw[:, ::-1]], axis=1)\n        signal_train.append(signal)\n\n        os.remove(\"/kaggle/working/\" + str(planet_id) + f\"_AIRS-CH0_signal_{VERSION}.npy\")\n        os.remove(\"/kaggle/working/\" + str(planet_id) + f\"_FGS1_signal_{VERSION}.npy\")\n\n    signal_train = np.array(signal_train)\n    np.save(f\"signal_{VERSION}.npy\", signal_train, allow_pickle=False)\n\n    print(\"Processing complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:53:18.127969Z","iopub.execute_input":"2025-11-14T16:53:18.128228Z","iopub.status.idle":"2025-11-14T16:53:18.137619Z","shell.execute_reply.started":"2025-11-14T16:53:18.128212Z","shell.execute_reply":"2025-11-14T16:53:18.136783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = f\"/kaggle/input/signal-v2-1-npy/signal_{VERSION} (1).npy\"\nos.environ[\"PREPROCESS_MODE\"] = \"train\"\n\n# Check if preprocessed data already exists\nif not os.path.exists(data_path):\n    print(\"Preprocessed data not found. Running preprocessing...\")\n\n    # Set environment variable for your script\n    os.environ[\"PREPROCESS_MODE\"] = \"train\"\n\n    # Run the preprocessing script\n    !python preprocess.py\n\n    # (Assuming preprocess.py saves 'signal_{VERSION}.npy')\nelse:\n    print(\"Preprocessed data already exists. Skipping preprocessing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T16:53:28.103022Z","iopub.execute_input":"2025-11-14T16:53:28.103763Z","iopub.status.idle":"2025-11-14T17:35:10.809946Z","shell.execute_reply.started":"2025-11-14T16:53:28.103708Z","shell.execute_reply":"2025-11-14T17:35:10.809203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train = np.load(f\"/kaggle/working/signal_v2.npy\")\ndata_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:37:16.608076Z","iopub.execute_input":"2025-11-14T17:37:16.608364Z","iopub.status.idle":"2025-11-14T17:37:17.019929Z","shell.execute_reply.started":"2025-11-14T17:37:16.608340Z","shell.execute_reply":"2025-11-14T17:37:17.019062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"check, done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:37:20.314072Z","iopub.execute_input":"2025-11-14T17:37:20.314346Z","iopub.status.idle":"2025-11-14T17:37:20.318421Z","shell.execute_reply.started":"2025-11-14T17:37:20.314324Z","shell.execute_reply":"2025-11-14T17:37:20.317677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile view_signal.py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom scipy import stats, ndimage\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up plotting style\nplt.style.use('dark_background')\nsns.set_palette(\"viridis\")\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nCONFIG = {\n    # Data paths\n    'base_path': \"/kaggle/input/ariel-data-challenge-2025\",\n    'data_split': \"test\",  # \"train\" or \"test\"\n    \n    # Target selection\n    'target_id': \"1103775\",  # Specific target ID or None for first available\n    \n    # Instruments to analyze\n    'instruments': ['FGS1', 'AIRS-CH0'],\n    \n    # Calibration types to analyze\n    'calibration_types': ['dark', 'flat', 'linear_corr', 'dead', 'read'],\n    \n    # Analysis options\n    'analyze_calibration': True,\n    'analyze_science': True,\n    'analyze_atmosphere': True,\n    \n    # Atmospheric analysis parameters\n    'spectral_rows': (10, 22),  # Rows to extract spectrum from AIRS-CH0\n    'out_of_transit_frames': (0, 10),  # Frame range for baseline\n    'in_transit_frames': (20, 30),  # Frame range for transit\n    \n    # Detector specifications\n    'detector_specs': {\n        'FGS1': {\n            'shape': (32, 32),\n            'band': 'Visible (0.5-0.8 Œºm)',\n            'purpose': 'Fine Guidance',\n            'cmap': 'magma',\n            'wavelength_range': None\n        },\n        'AIRS-CH0': {\n            'shape': (32, 356),\n            'band': 'NIR (0.95-1.95 Œºm)',\n            'purpose': 'Spectroscopy',\n            'cmap': 'inferno',\n            'wavelength_range': (0.95, 1.95)\n        }\n    },\n    \n    # Molecular absorption bands (for atmospheric analysis)\n    'molecular_bands': {\n        'H2O': [1.1, 1.2, 1.35, 1.5, 1.87],\n        'CH4': [1.6, 1.8]\n    },\n    \n    # Visualization options\n    'verbose': False,  # Set to True for debug output\n    'show_plots': True\n}\n\n# =============================================================================\n# CALIBRATION DATA ANALYSIS\n# =============================================================================\n\nclass CalibrationAnalyzer:\n    def __init__(self, config):\n        self.config = config\n        self.base_path = config['base_path']\n        self.data_split = config['data_split']\n        self.detector_specs = config['detector_specs']\n    \n    def load_calibration_data(self, target_id, instrument, cal_type):\n        \"\"\"Load specific calibration data\"\"\"\n        cal_path = Path(self.base_path) / self.data_split / target_id / \\\n                   f\"{instrument}_calibration_0\" / f\"{cal_type}.parquet\"\n        \n        if cal_path.exists():\n            return pd.read_parquet(cal_path)\n        return None\n    \n    def analyze_calibration_frame(self, data, detector_shape):\n        \"\"\"Analyze a single calibration frame\"\"\"\n        if data is None or data.empty:\n            return None, None\n        \n        try:\n            # Handle various data formats\n            if data.shape == detector_shape:\n                frame = data.values\n            elif len(data.shape) == 2 and data.shape[1:] == detector_shape:\n                frame = data.iloc[0].values if hasattr(data, 'iloc') else data[0]\n            elif len(data.shape) == 2 and data.shape[0] > detector_shape[0]:\n                frames_per_stack = data.shape[0] // detector_shape[0]\n                frame = data.values.reshape(frames_per_stack, *detector_shape).mean(axis=0)\n            elif data.shape[1] == np.prod(detector_shape):\n                frame = data.iloc[0].values.reshape(detector_shape)\n            else:\n                frame = data.values\n            \n            # Handle boolean data (dead pixel masks)\n            if frame.dtype == bool:\n                frame = frame.astype(int)\n            \n            frame = np.array(frame, dtype=float)\n            \n            stats_dict = {\n                'mean': np.mean(frame),\n                'median': np.median(frame),\n                'std': np.std(frame),\n                'min': np.min(frame),\n                'max': np.max(frame),\n                'hot_pixels': np.sum(frame > np.percentile(frame, 99.9)) if frame.max() > frame.min() else 0,\n                'dead_pixels': np.sum(frame == 0),\n                'shape': frame.shape\n            }\n            \n            return frame, stats_dict\n            \n        except Exception as e:\n            if self.config['verbose']:\n                print(f\"Error processing calibration frame: {e}\")\n            return None, None\n    \n    def plot_calibration_suite(self, target_id, instrument):\n        \"\"\"Comprehensive calibration analysis for one instrument\"\"\"\n        cal_types = self.config['calibration_types']\n        detector_shape = self.detector_specs[instrument]['shape']\n        band_info = self.detector_specs[instrument]['band']\n        \n        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n        fig.suptitle(f'üõ∏ {instrument} Calibration Suite - Target {target_id}\\n{band_info}', \n                    fontsize=16, y=0.98)\n        \n        axes = axes.flatten()\n        \n        for i, cal_type in enumerate(cal_types):\n            ax = axes[i]\n            cal_data = self.load_calibration_data(target_id, instrument, cal_type)\n            \n            if cal_data is not None and not cal_data.empty:\n                frame, stats = self.analyze_calibration_frame(cal_data, detector_shape)\n                \n                if frame is not None:\n                    cmap = {\n                        'dark': 'inferno',\n                        'flat': 'plasma',\n                        'linear_corr': 'viridis',\n                        'dead': 'binary',\n                        'read': 'magma'\n                    }.get(cal_type, 'viridis')\n                    \n                    im = ax.imshow(frame, cmap=cmap, aspect='auto')\n                    plt.colorbar(im, ax=ax, fraction=0.046)\n                    \n                    stats_text = f\"Œº={stats['mean']:.1f}\\nœÉ={stats['std']:.1f}\\nHot:{stats['hot_pixels']}\\nDead:{stats['dead_pixels']}\"\n                    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"black\", alpha=0.8),\n                           verticalalignment='top', fontsize=9, color='white')\n                else:\n                    ax.text(0.5, 0.5, f\"‚ùå Processing Error\", ha='center', va='center', \n                           transform=ax.transAxes, fontsize=12, color='red')\n            else:\n                ax.text(0.5, 0.5, f\"‚ùå No Data\", ha='center', va='center', \n                       transform=ax.transAxes, fontsize=12, color='red')\n            \n            ax.set_title(f\"{cal_type.upper()} Calibration\", fontsize=12)\n            ax.set_xlabel(\"Pixel Column\")\n            ax.set_ylabel(\"Pixel Row\")\n        \n        if len(cal_types) < len(axes):\n            axes[-1].remove()\n        \n        plt.tight_layout()\n        if self.config['show_plots']:\n            plt.show()\n        \n        return True\n\n# =============================================================================\n# SCIENCE DATA VISUALIZATION\n# =============================================================================\n\nclass ScienceDataVisualizer:\n    def __init__(self, config):\n        self.config = config\n        self.base_path = config['base_path']\n        self.data_split = config['data_split']\n        self.detector_specs = config['detector_specs']\n    \n    def load_science_data(self, target_id, instrument):\n        \"\"\"Load science observation data\"\"\"\n        sci_path = Path(self.base_path) / self.data_split / target_id / \\\n                   f\"{instrument}_signal_0.parquet\"\n        \n        if sci_path.exists():\n            return pd.read_parquet(sci_path)\n        return None\n    \n    def analyze_time_series(self, data, detector_shape):\n        \"\"\"Analyze temporal behavior of observations\"\"\"\n        if data is None or data.empty:\n            return None, None\n        \n        try:\n            expected_pixels = np.prod(detector_shape)\n            \n            if data.shape[1] == expected_pixels:\n                flux_series = data.sum(axis=1).values\n                \n                frame_stats = []\n                num_frames = min(len(data), 50)\n                \n                for i in range(num_frames):\n                    frame = data.iloc[i].values.reshape(detector_shape)\n                    frame_stats.append({\n                        'frame': i,\n                        'total_flux': np.sum(frame),\n                        'mean_flux': np.mean(frame),\n                        'max_flux': np.max(frame),\n                        'std_flux': np.std(frame)\n                    })\n                \n                stats_df = pd.DataFrame(frame_stats)\n                return flux_series, stats_df\n            else:\n                return None, None\n                \n        except Exception as e:\n            if self.config['verbose']:\n                print(f\"Error in time series analysis: {e}\")\n            return None, None\n    \n    def plot_exoplanet_analysis(self, target_id):\n        \"\"\"Comprehensive exoplanet observation analysis\"\"\"\n        fig = plt.figure(figsize=(24, 16))\n        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n        \n        fig.suptitle(f'üåå Exoplanet Observation Analysis - Target {target_id}', \n                    fontsize=18, y=0.98)\n        \n        instruments = self.config['instruments']\n        \n        for idx, instrument in enumerate(instruments):\n            data = self.load_science_data(target_id, instrument)\n            if data is None:\n                continue\n            \n            detector_shape = self.detector_specs[instrument]['shape']\n            cmap = self.detector_specs[instrument]['cmap']\n            band = self.detector_specs[instrument]['band']\n            \n            # 1. First frame\n            ax1 = fig.add_subplot(gs[idx*2, 0])\n            try:\n                frame0 = data.iloc[0].values.reshape(detector_shape)\n                im1 = ax1.imshow(frame0, cmap=cmap, aspect='auto')\n                plt.colorbar(im1, ax=ax1, fraction=0.046)\n                ax1.set_title(f\"{instrument} - First Frame\\n{band}\", fontsize=11)\n            except Exception as e:\n                ax1.text(0.5, 0.5, f\"‚ùå Error\", ha='center', va='center', \n                        transform=ax1.transAxes, fontsize=10)\n            \n            # 2. Last frame\n            ax2 = fig.add_subplot(gs[idx*2, 1])\n            try:\n                frame_last = data.iloc[-1].values.reshape(detector_shape)\n                im2 = ax2.imshow(frame_last, cmap=cmap, aspect='auto')\n                plt.colorbar(im2, ax=ax2, fraction=0.046)\n                ax2.set_title(f\"{instrument} - Last Frame\", fontsize=11)\n            except Exception as e:\n                ax2.text(0.5, 0.5, f\"‚ùå Error\", ha='center', va='center', \n                        transform=ax2.transAxes, fontsize=10)\n            \n            # 3. Difference frame\n            ax3 = fig.add_subplot(gs[idx*2, 2])\n            try:\n                if 'frame0' in locals() and 'frame_last' in locals():\n                    diff_frame = frame_last - frame0\n                    im3 = ax3.imshow(diff_frame, cmap='RdBu_r', aspect='auto')\n                    plt.colorbar(im3, ax=ax3, fraction=0.046)\n                    ax3.set_title(f\"{instrument} - Difference\\n(Last - First)\", fontsize=11)\n            except Exception as e:\n                ax3.text(0.5, 0.5, f\"‚ùå Error\", ha='center', va='center', \n                        transform=ax3.transAxes, fontsize=10)\n            \n            # 4. Time series\n            flux_series, stats_df = self.analyze_time_series(data, detector_shape)\n            \n            if flux_series is not None:\n                ax4 = fig.add_subplot(gs[idx*2:(idx*2)+2, 3])\n                time_points = np.arange(len(flux_series))\n                ax4.plot(time_points, flux_series, 'o-', alpha=0.7, linewidth=2, markersize=3)\n                \n                if len(flux_series) > 10:\n                    poly_fit = np.polyfit(time_points, flux_series, 3)\n                    smooth_flux = np.polyval(poly_fit, time_points)\n                    ax4.plot(time_points, smooth_flux, 'r--', alpha=0.8, linewidth=2, \n                            label='Polynomial Fit')\n                    ax4.legend()\n                \n                ax4.set_title(f\"{instrument} - Flux Time Series\", fontsize=11)\n                ax4.set_xlabel(\"Frame Number\")\n                ax4.set_ylabel(\"Total Flux (counts)\")\n                ax4.grid(True, alpha=0.3)\n                \n                flux_normalized = flux_series / np.median(flux_series)\n                min_flux = np.min(flux_normalized)\n                if min_flux < 0.998:\n                    transit_depth = (1 - min_flux) * 100\n                    ax4.text(0.02, 0.98, f\"Transit Depth: {transit_depth:.3f}%\", \n                            transform=ax4.transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", \n                            facecolor=\"yellow\", alpha=0.8), verticalalignment='top')\n            \n            # 5. Mean frame\n            ax5 = fig.add_subplot(gs[idx*2+1, 0])\n            mean_frame = data.mean(axis=0).values.reshape(detector_shape)\n            im5 = ax5.imshow(mean_frame, cmap=cmap, aspect='auto')\n            plt.colorbar(im5, ax=ax5, fraction=0.046)\n            ax5.set_title(f\"{instrument} - Mean Frame\", fontsize=11)\n            \n            # 6. Noise map\n            ax6 = fig.add_subplot(gs[idx*2+1, 1])\n            std_frame = data.std(axis=0).values.reshape(detector_shape)\n            im6 = ax6.imshow(std_frame, cmap='hot', aspect='auto')\n            plt.colorbar(im6, ax=ax6, fraction=0.046)\n            ax6.set_title(f\"{instrument} - Noise Map (œÉ)\", fontsize=11)\n            \n            # 7. SNR map\n            ax7 = fig.add_subplot(gs[idx*2+1, 2])\n            snr_frame = np.divide(mean_frame, std_frame, \n                                out=np.zeros_like(mean_frame), where=std_frame!=0)\n            im7 = ax7.imshow(snr_frame, cmap='plasma', aspect='auto', vmin=0, vmax=50)\n            plt.colorbar(im7, ax=ax7, fraction=0.046)\n            ax7.set_title(f\"{instrument} - SNR Map\", fontsize=11)\n        \n        plt.tight_layout()\n        if self.config['show_plots']:\n            plt.show()\n        \n        return True\n\n# =============================================================================\n# ATMOSPHERIC COMPOSITION ANALYSIS\n# =============================================================================\n\nclass AtmosphericAnalyzer:\n    def __init__(self, config):\n        self.config = config\n        self.base_path = config['base_path']\n        self.data_split = config['data_split']\n        self.sci_viz = ScienceDataVisualizer(config)\n    \n    def analyze_composition(self, target_id):\n        \"\"\"Extract atmospheric composition from transit spectroscopy\"\"\"\n        \n        airs_data = self.sci_viz.load_science_data(target_id, 'AIRS-CH0')\n        \n        if airs_data is None:\n            if self.config['verbose']:\n                print(\"No AIRS-CH0 data available for atmospheric analysis\")\n            return None, None, None\n        \n        # Extract spectral configuration\n        spectral_rows = self.config['spectral_rows']\n        wl_range = self.config['detector_specs']['AIRS-CH0']['wavelength_range']\n        \n        # Reshape to (time, wavelength)\n        spectra = []\n        for i in range(len(airs_data)):\n            frame = airs_data.iloc[i].values.reshape(32, 356)\n            spectrum = np.sum(frame[spectral_rows[0]:spectral_rows[1], :], axis=0)\n            spectra.append(spectrum)\n        \n        spectra = np.array(spectra)\n        time_points = np.arange(len(spectra))\n        wavelengths = np.linspace(wl_range[0], wl_range[1], 356)\n        \n        self._plot_atmospheric_analysis(target_id, wavelengths, spectra, time_points)\n        \n        return wavelengths, spectra, time_points\n    \n    def _plot_atmospheric_analysis(self, target_id, wavelengths, spectra, time_points):\n        \"\"\"Generate atmospheric composition plots\"\"\"\n        \n        fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n        fig.suptitle(f'üåç Atmospheric Composition Analysis - Target {target_id}', fontsize=16)\n        \n        # 1. Transit light curves at key wavelengths\n        ax1 = axes[0, 0]\n        key_wavelengths = [50, 100, 150, 200, 250, 300]\n        colors = plt.cm.viridis(np.linspace(0, 1, len(key_wavelengths)))\n        \n        for wl_idx, color in zip(key_wavelengths, colors):\n            normalized_flux = spectra[:, wl_idx] / np.median(spectra[:, wl_idx])\n            ax1.plot(time_points, normalized_flux, 'o-', color=color, \n                    label=f'{wavelengths[wl_idx]:.2f} Œºm', alpha=0.8, markersize=3)\n        \n        ax1.set_xlabel('Frame Number')\n        ax1.set_ylabel('Normalized Flux')\n        ax1.set_title('Transit Depth vs Wavelength')\n        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        ax1.grid(True, alpha=0.3)\n        \n        # 2. Transmission spectrum\n        ax2 = axes[0, 1]\n        transit_depths = []\n        for wl_idx in range(356):\n            flux = spectra[:, wl_idx]\n            normalized = flux / np.median(flux)\n            min_flux = np.min(normalized)\n            transit_depth = (1 - min_flux) * 100\n            transit_depths.append(transit_depth)\n        \n        ax2.plot(wavelengths, transit_depths, 'b-', linewidth=2)\n        ax2.set_xlabel('Wavelength (Œºm)')\n        ax2.set_ylabel('Transit Depth (%)')\n        ax2.set_title('Transmission Spectrum')\n        ax2.grid(True, alpha=0.3)\n        \n        # Highlight molecular bands\n        molecular_bands = self.config['molecular_bands']\n        if 'H2O' in molecular_bands:\n            for wl in molecular_bands['H2O'][:2]:\n                if wavelengths[0] <= wl <= wavelengths[-1]:\n                    ax2.axvline(wl, alpha=0.3, color='blue', linestyle='--')\n        \n        if 'CH4' in molecular_bands:\n            for wl in molecular_bands['CH4']:\n                if wavelengths[0] <= wl <= wavelengths[-1]:\n                    ax2.axvline(wl, alpha=0.3, color='red', linestyle='--')\n        \n        # 3. Spectral evolution\n        ax3 = axes[1, 0]\n        im = ax3.imshow(spectra.T, aspect='auto', origin='lower', \n                        extent=[0, len(time_points), wavelengths[0], wavelengths[-1]],\n                        cmap='plasma')\n        plt.colorbar(im, ax=ax3, label='Flux (counts)')\n        ax3.set_xlabel('Frame Number')\n        ax3.set_ylabel('Wavelength (Œºm)')\n        ax3.set_title('Spectral Evolution During Transit')\n        \n        # 4. Relative absorption\n        ax4 = axes[1, 1]\n        out_range = self.config['out_of_transit_frames']\n        in_range = self.config['in_transit_frames']\n        \n        out_of_transit = np.mean(spectra[out_range[0]:out_range[1]], axis=0)\n        in_transit = np.mean(spectra[in_range[0]:in_range[1]], axis=0)\n        \n        relative_absorption = (out_of_transit - in_transit) / out_of_transit * 100\n        \n        ax4.plot(wavelengths, relative_absorption, 'g-', linewidth=2)\n        ax4.set_xlabel('Wavelength (Œºm)')\n        ax4.set_ylabel('Relative Absorption (%)')\n        ax4.set_title('Atmospheric Absorption Features')\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        if self.config['show_plots']:\n            plt.show()\n\n# =============================================================================\n# MAIN EXECUTION\n# =============================================================================\n\ndef run_analysis(config=None):\n    \"\"\"Main analysis pipeline\"\"\"\n    \n    if config is None:\n        config = CONFIG\n    \n    # Get target ID\n    data_path = Path(config['base_path']) / config['data_split']\n    \n    if not data_path.exists():\n        print(f\"‚ùå Path not found: {data_path}\")\n        return\n    \n    targets = [d.name for d in data_path.iterdir() if d.is_dir()]\n    \n    if not targets:\n        print(f\"‚ùå No targets found in {data_path}\")\n        return\n    \n    target_id = config['target_id']\n    if target_id is None:\n        target_id = targets[0]\n    elif target_id not in targets:\n        print(f\"‚ùå Target {target_id} not found. Using first available: {targets[0]}\")\n        target_id = targets[0]\n    \n    print(f\"üéØ Analyzing Target: {target_id} from {config['data_split']} split\")\n    \n    # Initialize analyzers\n    cal_analyzer = CalibrationAnalyzer(config)\n    sci_visualizer = ScienceDataVisualizer(config)\n    atm_analyzer = AtmosphericAnalyzer(config)\n    \n    # Run analyses\n    if config['analyze_calibration']:\n        for instrument in config['instruments']:\n            cal_analyzer.plot_calibration_suite(target_id, instrument)\n    \n    if config['analyze_science']:\n        sci_visualizer.plot_exoplanet_analysis(target_id)\n    \n    if config['analyze_atmosphere']:\n        atm_analyzer.analyze_composition(target_id)\n    \n    print(\"‚úÖ Analysis Complete!\")\n\n# =============================================================================\n# RUN\n# =============================================================================\n\n# if __name__ == \"__main__\":\n#     run_analysis(CONFIG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:37:27.628626Z","iopub.execute_input":"2025-11-14T17:37:27.628913Z","iopub.status.idle":"2025-11-14T17:37:27.642992Z","shell.execute_reply.started":"2025-11-14T17:37:27.628893Z","shell.execute_reply":"2025-11-14T17:37:27.642237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exec(open('view_signal.py', 'r').read())\nCONFIG['base_path'] = '/kaggle/input/ariel-data-challenge-2025'\nCONFIG['data_split'] = 'train'   # or 'test' depending on stage\nCONFIG['target_id'] = '1010375142'  # update to your target folder name\nCONFIG['show_plots'] = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:37:36.635838Z","iopub.execute_input":"2025-11-14T17:37:36.636767Z","iopub.status.idle":"2025-11-14T17:37:36.970507Z","shell.execute_reply.started":"2025-11-14T17:37:36.636739Z","shell.execute_reply":"2025-11-14T17:37:36.969941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_analysis(CONFIG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:37:41.559976Z","iopub.execute_input":"2025-11-14T17:37:41.560596Z","iopub.status.idle":"2025-11-14T17:38:05.933847Z","shell.execute_reply.started":"2025-11-14T17:37:41.560570Z","shell.execute_reply":"2025-11-14T17:38:05.933083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# ---------------------------------------------------------\n# CONFIGURATION\n# ---------------------------------------------------------\n\nDATA_ROOT = \"/kaggle/input/ariel-data-challenge-2025\"\nSIGNAL_FILE = f\"/kaggle/working/signal_v2.npy\"   # preprocessed data file (all planets)\nSTAR_INFO = f\"{DATA_ROOT}/train_star_info.csv\"\n\n# ---------------------------------------------------------\n# LOAD UTILITIES\n# ---------------------------------------------------------\n\ndef load_preprocessed_signal(signal_path=SIGNAL_FILE):\n    \"\"\"Load preprocessed (npy) signal file\"\"\"\n    signal = np.load(signal_path)\n    print(f\"‚úÖ Loaded preprocessed signal: shape {signal.shape}\")\n    return signal\n\ndef load_raw_signal(planet_id, base_path=DATA_ROOT, split=\"train\"):\n    \"\"\"Load raw parquet signals for one planet\"\"\"\n    base = Path(base_path) / split / str(planet_id)\n    fgs1_path = base / \"FGS1_signal_0.parquet\"\n    airs_path = base / \"AIRS-CH0_signal_0.parquet\"\n    if not fgs1_path.exists():\n        raise FileNotFoundError(f\"No data found for {planet_id}\")\n\n    fgs1 = pd.read_parquet(fgs1_path).values\n    airs = pd.read_parquet(airs_path).values\n\n    # Collapse FGS1 to 1D light curve (sum of all detector pixels)\n    fgs1 = fgs1.sum(axis=1)\n    return fgs1, airs\n\n# ---------------------------------------------------------\n# VISUALIZATION\n# ---------------------------------------------------------\ndef plot_signals(fgs1, airs, title_prefix=\"Planet\"):\n    \"\"\"Visualize FGS1 (1D) and AIRS-CH0 (1D or 2D) signals\"\"\"\n    import matplotlib.pyplot as plt\n\n    # --- Create subplots dynamically ---\n    if airs.ndim == 1:\n        fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n    else:\n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n    # --- 1Ô∏è‚É£ FGS1 Light Curve ---\n    ax[0].plot(fgs1, color='cyan', lw=1.5)\n    ax[0].set_title(f\"{title_prefix} ‚Äî FGS1 Light Curve\", fontsize=12)\n    ax[0].set_xlabel(\"Time bin\")\n    ax[0].set_ylabel(\"Flux (counts)\")\n    ax[0].grid(alpha=0.3)\n\n    # --- 2Ô∏è‚É£ AIRS Visualization ---\n    if airs.ndim == 2:\n        im = ax[1].imshow(airs.T, aspect='auto', origin='lower', cmap='inferno')\n        ax[1].set_title(f\"{title_prefix} ‚Äî AIRS-CH0 Spectrum\", fontsize=12)\n        ax[1].set_xlabel(\"Time bin\")\n        ax[1].set_ylabel(\"Wavelength bin\")\n        plt.colorbar(im, ax=ax[1], fraction=0.046, label=\"Flux Intensity\")\n    elif airs.ndim == 1:\n        ax[1].plot(airs, color='orange', lw=1.5)\n        ax[1].set_title(f\"{title_prefix} ‚Äî AIRS-CH0 Light Curve\", fontsize=12)\n        ax[1].set_xlabel(\"Time bin\")\n        ax[1].set_ylabel(\"Flux (counts)\")\n        ax[1].grid(alpha=0.3)\n    else:\n        raise ValueError(f\"Unexpected AIRS shape: {airs.shape}\")\n\n    plt.tight_layout()\n    plt.show()\n\n# ---------------------------------------------------------\n# MAIN FUNCTION\n# ---------------------------------------------------------\n\ndef visualize_planet(planet_idx=0, source=\"preprocessed\"):\n    \"\"\"\n    Visualize one planet's signals.\n    source = 'preprocessed' or 'raw'\n    \"\"\"\n    if source == \"preprocessed\":\n        # Load npy file and get by index\n        signal = load_preprocessed_signal()\n        fgs1, airs = signal[planet_idx, 0], signal[planet_idx, 1]\n        title_prefix = f\"Preprocessed Planet {planet_idx}\"\n\n    elif source == \"raw\":\n        # Map index to planet_id using star_info\n        star_info = pd.read_csv(STAR_INFO, index_col=\"planet_id\")\n        planet_ids = star_info.index.to_list()\n        planet_id = planet_ids[planet_idx]\n\n        fgs1, airs = load_raw_signal(planet_id)\n        title_prefix = f\"Raw Planet ID {planet_id} (idx={planet_idx})\"\n\n    else:\n        raise ValueError(\"source must be 'preprocessed' or 'raw'\")\n\n    plot_signals(fgs1, airs, title_prefix)\n\n\n\ndef compare_planet_signals(planet_idx, signal_v2_path=SIGNAL_FILE, raw_base_dir=\"/kaggle/input/ariel-data-challenge-2025/train\"):\n    \"\"\"\n    Compare raw and preprocessed signals for a given planet index.\n    Automatically handles 1D vs 2D AIRS data.\n    \"\"\"\n\n    # --- Load preprocessed data ---\n    signal = np.load(signal_v2_path, allow_pickle=True)\n    prep_fgs1, prep_airs = signal[planet_idx, 0], signal[planet_idx, 1]\n\n    # --- Identify planet ID from folder structure ---\n    planet_id = sorted(os.listdir(raw_base_dir))[planet_idx]\n    print(f\"ü™ê Viewing Planet ID: {planet_id}\")\n\n    # --- Load raw data ---\n    raw_fgs1_path = os.path.join(raw_base_dir, planet_id, \"FGS1_signal_0.parquet\")\n    raw_airs_path = os.path.join(raw_base_dir, planet_id, \"AIRS-CH0_signal_0.parquet\")\n\n    raw_fgs1 = pd.read_parquet(raw_fgs1_path).values\n    raw_airs = pd.read_parquet(raw_airs_path).values\n\n    # Flatten if needed\n    if raw_fgs1.ndim > 1:\n        raw_fgs1 = raw_fgs1.flatten()\n\n    # --- Plot FGS1 (1D) ---\n    plt.figure(figsize=(10, 4))\n    plt.plot(raw_fgs1, label=\"Raw FGS1\", alpha=0.6, color=\"orange\")\n    plt.plot(prep_fgs1, label=\"Preprocessed FGS1\", alpha=0.8, color=\"cyan\")\n    plt.title(f\"FGS1 Light Curve Comparison ‚Äî Planet {planet_id}\")\n    plt.xlabel(\"Time bins\")\n    plt.ylabel(\"Flux\")\n    plt.legend()\n    plt.grid(alpha=0.3)\n    plt.show()\n\n    # --- Plot AIRS (handles 1D or 2D automatically) ---\n    if prep_airs.ndim == 1:\n        fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n        mid_band = raw_airs.shape[1] // 2\n\n        ax.plot(raw_airs[:, mid_band], label='Raw Central Band', alpha=0.6, color='orange')\n        ax.plot(prep_airs, label='Preprocessed (Averaged)', alpha=0.8, color='cyan')\n        ax.set_title(f\"AIRS-CH0 Comparison ‚Äî Planet {planet_id}\")\n        ax.set_xlabel(\"Time bin\")\n        ax.set_ylabel(\"Flux\")\n        ax.legend()\n        ax.grid(alpha=0.3)\n        plt.show()\n    else:\n        fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n        im0 = ax[0].imshow(raw_airs.T, aspect='auto', origin='lower', cmap='magma')\n        ax[0].set_title(\"Raw AIRS-CH0\")\n        plt.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\n        im1 = ax[1].imshow(prep_airs.T, aspect='auto', origin='lower', cmap='magma')\n        ax[1].set_title(\"Preprocessed AIRS-CH0\")\n        plt.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\n        mid_band = raw_airs.shape[1] // 2\n        ax[2].plot(raw_airs[:, mid_band], label='Raw', alpha=0.6, color='orange')\n        ax[2].plot(prep_airs[:, mid_band], label='Preprocessed', alpha=0.8, color='cyan')\n        ax[2].set_title(\"Central Band Comparison\")\n        ax[2].set_xlabel(\"Time bin\")\n        ax[2].set_ylabel(\"Flux\")\n        ax[2].legend()\n        ax[2].grid(alpha=0.3)\n\n        fig.suptitle(f\"AIRS-CH0 Comparison ‚Äî Planet {planet_id}\", fontsize=13)\n        plt.tight_layout()\n        plt.show()\n\ndef plot_fgs1_comparison(raw_fgs1, prep_fgs1, planet_id):\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Normalize preprocessed signal (since it's usually near 1)\n    norm_raw = raw_fgs1 / np.nanmean(raw_fgs1)\n    norm_prep = prep_fgs1 / np.nanmean(prep_fgs1)\n\n    fig, ax1 = plt.subplots(figsize=(10, 4))\n\n    # Left axis: normalized flux comparison\n    ax1.plot(norm_raw, label=\"Raw (normalized)\", alpha=0.5, color=\"orange\")\n    ax1.plot(norm_prep, label=\"Preprocessed\", alpha=0.8, color=\"cyan\")\n    ax1.set_xlabel(\"Time bins\")\n    ax1.set_ylabel(\"Normalized Flux\")\n    ax1.grid(alpha=0.3)\n    ax1.legend()\n\n    # Right axis: show raw counts scale for context\n    ax2 = ax1.twinx()\n    ax2.set_ylabel(\"Raw Counts\", color=\"orange\", alpha=0.6)\n    ax2.tick_params(axis='y', labelcolor='orange', colors='orange')\n\n    plt.title(f\"FGS1 Light Curve Comparison ‚Äî Planet {planet_id}\")\n    plt.tight_layout()\n    plt.show()\n\n# ---------------------------------------------------------\n# EXAMPLE USAGE\n# ---------------------------------------------------------\n\nif __name__ == \"__main__\":\n    # Example: view planet #0 in preprocessed mode\n    visualize_planet(0, source=\"preprocessed\")\n\n    # Example: view planet #0 in raw mode (from parquet)\n    visualize_planet(0, source=\"raw\")\n\n    compare_planet_signals(\n        planet_idx=0\n    )\n    # plot_fgs1_comparison(raw_fgs1, prep_fgs1, planet_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:38:41.085381Z","iopub.execute_input":"2025-11-14T17:38:41.085897Z","iopub.status.idle":"2025-11-14T17:40:15.606697Z","shell.execute_reply.started":"2025-11-14T17:38:41.085871Z","shell.execute_reply":"2025-11-14T17:40:15.605987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef compare_planet_signals(planet_idx, signal_v2_path='/kaggle/working/signal_v2.npy', raw_base_dir=\"./train\"):\n    \"\"\"\n    Compare raw and preprocessed signals for a given planet index.\n    \n    - Loads raw .parquet data from the ARIEL dataset folder\n    - Loads preprocessed data from signal_v2.npy\n    - Overlays FGS1 and AIRS-CH0 signals for visual comparison\n    \"\"\"\n\n    # --- Load preprocessed data ---\n    signal = np.load(signal_v2_path, allow_pickle=True)\n    prep_fgs1, prep_airs = signal[planet_idx, 0], signal[planet_idx, 1]\n\n    # --- Identify planet ID from folder structure ---\n    planet_id = sorted(os.listdir(raw_base_dir))[planet_idx]\n    print(f\"ü™ê Viewing Planet ID: {planet_id}\")\n\n    # --- Load raw data ---\n    raw_fgs1_path = os.path.join(raw_base_dir, planet_id, \"FGS1_signal_0.parquet\")\n    raw_airs_path = os.path.join(raw_base_dir, planet_id, \"AIRS-CH0_signal_0.parquet\")\n\n    raw_fgs1 = pd.read_parquet(raw_fgs1_path).values\n    raw_airs = pd.read_parquet(raw_airs_path).values\n\n    # Flatten if needed\n    if raw_fgs1.ndim > 1:\n        raw_fgs1 = raw_fgs1.flatten()\n\n    # --- Plot FGS1 (1D) ---\n    plt.figure(figsize=(10, 4))\n    plt.plot(raw_fgs1, label=\"Raw FGS1\", alpha=0.6, color=\"orange\")\n    plt.plot(prep_fgs1, label=\"Preprocessed FGS1\", alpha=0.8, color=\"cyan\")\n    plt.title(f\"FGS1 Light Curve Comparison ‚Äî Planet {planet_id}\")\n    plt.xlabel(\"Time bins\")\n    plt.ylabel(\"Flux\")\n    plt.legend()\n    plt.grid(alpha=0.3)\n    plt.show()\n\n    # --- Plot AIRS-CH0 (2D heatmap and overlay) ---\n    fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n\n    im0 = ax[0].imshow(raw_airs.T, aspect='auto', origin='lower', cmap='magma')\n    ax[0].set_title(\"Raw AIRS-CH0\")\n    plt.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\n    im1 = ax[1].imshow(prep_airs.T, aspect='auto', origin='lower', cmap='magma')\n    ax[1].set_title(\"Preprocessed AIRS-CH0\")\n    plt.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\n    mid_band = raw_airs.shape[1] // 2\n    ax[2].plot(raw_airs[:, mid_band], label='Raw', alpha=0.6, color='orange')\n    ax[2].plot(prep_airs[:, mid_band], label='Preprocessed', alpha=0.8, color='cyan')\n    ax[2].set_title(\"Central Band Comparison\")\n    ax[2].set_xlabel(\"Time bin\")\n    ax[2].set_ylabel(\"Flux\")\n    ax[2].legend()\n    ax[2].grid(alpha=0.3)\n\n    fig.suptitle(f\"AIRS-CH0 Comparison ‚Äî Planet {planet_id}\", fontsize=13)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:40:48.075274Z","iopub.execute_input":"2025-11-14T17:40:48.075598Z","iopub.status.idle":"2025-11-14T17:40:48.085898Z","shell.execute_reply.started":"2025-11-14T17:40:48.075565Z","shell.execute_reply":"2025-11-14T17:40:48.085180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_adc_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/adc_info.csv')\ntrain_star_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/train_star_info.csv')\n\ntrain_labels = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/train.csv',\n                           index_col='planet_id')\nwavelengths = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/wavelengths.csv')\naxis_info = pd.read_parquet('/kaggle/input/ariel-data-challenge-2025/axis_info.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:40:51.582413Z","iopub.execute_input":"2025-11-14T17:40:51.582687Z","iopub.status.idle":"2025-11-14T17:40:51.809663Z","shell.execute_reply.started":"2025-11-14T17:40:51.582667Z","shell.execute_reply":"2025-11-14T17:40:51.808994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import savgol_filter\n\n\nMODEL_VERSION = \"v2\"\nPRE_BINNED_TIME = 15\n\nROOT = \"/kaggle/input/ariel-data-challenge-2025/\"\n\n\n# find transit zones\ndef phase_detector(signal_orig, smooth_window=11):\n    signal = signal_orig.reshape(-1, 1).mean(-1)\n    signal = savgol_filter(signal, smooth_window, 2)  # smooth\n    first_derivative = np.gradient(signal)\n    second_derivative = savgol_filter(np.gradient(savgol_filter(first_derivative, 41, 2)), 41, 2)\n\n    local_min = (np.diff(np.sign(np.diff(second_derivative))) > 0).nonzero()[0] + 1\n    local_max = (np.diff(np.sign(np.diff(second_derivative))) < 0).nonzero()[0] + 1\n    \n    if len(local_min) >= 2:\n        top2_min_indices = local_min[np.argsort(second_derivative[local_min])[:2]]\n    else:\n        top2_min_indices = local_min\n    \n    if len(local_max) >= 2:\n        top2_max_indices = local_max[np.argsort(second_derivative[local_max])[-2:]]\n    else:\n        top2_max_indices = local_max\n    top2_min_indices.sort()\n    top2_max_indices.sort()\n\n    # 4 extrema of the 2nd derivative and 2 of the 1st\n    phase1 = top2_min_indices[0]\n    phase2 = top2_max_indices[0]\n    phase3 = top2_max_indices[1]\n    phase4 = top2_min_indices[1]\n    phase5 = np.argmin(first_derivative)\n    phase6 = np.argmax(first_derivative)\n\n    return phase1, phase2, phase3, phase4, phase5, phase6\n\n\ndef get_breakpoints(x, smooth=19):\n    bp1 = np.zeros(x.shape[0], dtype=np.int32)\n    bp2 = np.zeros(x.shape[0], dtype=np.int32)\n    bp3 = np.zeros(x.shape[0], dtype=np.int32)\n    bp4 = np.zeros(x.shape[0], dtype=np.int32)\n    bp5 = np.zeros(x.shape[0], dtype=np.int32)\n    bp6 = np.zeros(x.shape[0], dtype=np.int32)\n    for i in range(x.shape[0]):\n        signal = x[i]\n        p1, p2, p3, p4, p5, p6 = phase_detector(\n            signal, smooth_window=smooth\n        )\n        bp1[i] = p1\n        bp2[i] = p2\n        bp3[i] = p3\n        bp4[i] = p4\n        bp5[i] = p5\n        bp6[i] = p6\n        \n    return [bp1, bp2, bp3, bp4, bp5, bp6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:40:58.412229Z","iopub.execute_input":"2025-11-14T17:40:58.412881Z","iopub.status.idle":"2025-11-14T17:40:58.554605Z","shell.execute_reply.started":"2025-11-14T17:40:58.412853Z","shell.execute_reply":"2025-11-14T17:40:58.553753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile feature_engineering.py\nfrom scipy.signal import savgol_filter\nimport warnings\nfrom scipy.stats import kurtosis, skew\nfrom scipy.signal import medfilt\nfrom scipy.optimize import curve_fit\nfrom scipy.interpolate import interp1d\nfrom numpy.polynomial import Polynomial\nfrom scipy.optimize import least_squares, minimize\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.ndimage import median_filter\n\n\nwarnings.simplefilter(\"ignore\")\n\nA_BINNING = 15\n\n# threshold values for identifying outliers\nbad_low = 20\nbad_up = 354\n\nbuf = 15  # for lower1, upper1\nbuf1 = 10  # for lower, mid1, mid2, upper\n\n\ndef calculate_weights(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    \"\"\"\n    Calculating weights for averaging based on SNR\n    \"\"\"\n    max_len = a.shape[0] - 1\n    if is_outlier:\n        return np.ones_like(a[0]) / a.shape[-1]\n    else:\n        y_combined = np.concatenate([a[:max(lower - buf1, 1), :], a[min(upper + buf1, max_len):, :]], axis=0)\n        ratio = y_combined.mean(0) / y_combined.std(0)\n        return ratio / ratio.sum()\n\n\ndef calc_for_outliers(a, lower, upper):\n    \"\"\"\n    Estimating transit depth for outlier cases\n    \"\"\"\n    max_len = len(a) - 1\n            \n    if lower + buf < upper - buf:\n        obs = a[lower + buf : upper - buf].mean()\n    else:\n        obs = a[lower : upper].mean()\n\n    if lower - buf >= 10 and max_len - upper - buf >= 10:\n        unobs = (np.median(a[:max(lower - buf, 1)]) + np.median(a[min(upper + buf, max_len):])) / 2\n    elif lower >= max_len - upper:\n        unobs = np.median(a[:max(lower - buf, 1)])\n    else:\n        unobs = np.median(a[min(upper + buf, max_len):])\n\n    arr1 = 1 - (obs / unobs)\n    arr2 = 1 - a[(lower + upper) // 2] / unobs\n\n    return arr1, arr2\n\n\ndef calc_err(x_combined, y_combined, degree):\n    \"\"\"\n    Calculating the error to obtain the optimal polynomial degree\n    \"\"\"\n    max_len = 374 # hardcoded for BINNING=15\n    \n    poly_guess = np.polyfit(x_combined, y_combined, degree)\n    inter = np.polyval(poly_guess, np.arange(max_len + 1))\n    err = mean_squared_error(y_combined, inter[x_combined], squared=False)\n\n    # penalizing rmse, high polynomial degree and small number of points in curve fitting.\n    return err * degree**(1 - len(x_combined) / max_len)\n\n    \ndef calc_depth_and_detrend(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False, unstable=False, fixed_degree=None):\n    \"\"\"\n    Main function for transit depth estimation and detrending\n    \n    Parameters:\n        - a: 1d numpy array of observation points\n        - lower, mid1, mid2, upper, lower1, upper1: transit boundary points\n        - is_outlier: boolean flag indicating if the data point is an outlier\n        - unstable: if True, don't use curve fitting\n        - fixed_degree: if not None, use provided degree; otherwise, find optimal degree\n    \n    Returns:\n        - (arr1, arr2): tuple containing the averaged transit depth and the transit depth at mid-transit\n    \"\"\"\n    max_len = len(a) - 1\n    degree = 3\n    \n    if is_outlier:\n        a /= a.mean()\n        arr1, arr2 = calc_for_outliers(a, lower1, upper1)\n    else:\n        a /= a.mean()\n\n        # region outside the transit\n        x_combined = np.concatenate([np.arange(max(lower - buf1, 1)), np.arange(min(upper + buf1, max_len), max_len + 1)], axis=-1)\n        y_combined = a[x_combined]\n            \n        if fixed_degree is None: # find the optimal degre\n            best_val = 10**100\n            for j in [1, 2, 3, 4, 5]:\n                if calc_err(x_combined, y_combined, j) < best_val:\n                    best_val = calc_err(x_combined, y_combined, j)\n                    degree = j\n        else:\n            degree = fixed_degree\n\n        obs = a[mid1 : mid2]\n        \n        if unstable: # no curve fitting\n            unobs = y_combined.mean()\n        else:\n            poly_guess = np.polyfit(x_combined, y_combined, degree)         \n            inter = np.polyval(poly_guess, np.arange(max_len + 1))\n                \n            a /= inter\n            inter /= inter\n            unobs = inter[mid1 : mid2]\n\n        arr1 = 1 - np.mean(obs / unobs)\n        arr2 = 1 - a[(lower1 + upper1) // 2] / unobs.mean()\n                \n\n    if np.isnan(arr1):\n        arr1 = 0\n    if np.isnan(arr2):\n        arr2 = 0\n        \n    return arr1, arr2\n\n\ndef calc_slope(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    \"\"\"\n    Calculate transit wall steepness as slope between contact points\n    \"\"\"\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        return ((a[mid1] - a[lower]) / (mid1 - lower) - (a[upper] - a[mid2]) / (upper - mid2)) / 2\n\n\ndef calc_slope_2(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    \"\"\"\n    Calculate transit bottom curvature as slope between mid-transit \n    and contact point\n    \"\"\"\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        if not (mid1 < mid_ind < mid2):\n            return 0\n        return ((a[mid_ind] - a[mid1]) / (mid_ind - mid1) - (a[mid2] - a[mid_ind]) / (mid2 - mid_ind)) / 2\n\n\n# calcluating slopes for unsimmetric cases\ndef calc_slope_2_left(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper):\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        if not (mid1 < mid_ind < mid2):\n            return 0\n        return (a[mid_ind] - a[mid1]) / (mid_ind - mid1)\n\n\ndef calc_slope_2_right(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper):\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        if not (mid1 < mid_ind < mid2):\n            return 0\n        return (a[mid2] - a[mid_ind]) / (mid2 - mid_ind)\n\n\n# gradient slopes\ndef calc_curv_left(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        a = savgol_filter(np.gradient(a), 41, 1)\n        return (a[mid_ind] - a[mid1]) / (mid_ind - mid1)\n\n\ndef calc_curv_right(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        a = savgol_filter(np.gradient(a), 41, 1)\n        return (a[mid2] - a[mid_ind]) / (mid2 - mid_ind)\n\n\ndef calc_perc(a, lower, upper, q, is_outlier=False):\n    \"\"\"\n    Calculate the percentile of the transit depth, assumes the input signal is already detrended\n    \"\"\"\n    if is_outlier:\n        return 0\n    return np.quantile(1 - a[lower : upper], q)\n    \n\ndef feature_engineering(star_info, data):\n    \"\"\"\n    Prepares features for training or inference\n    \n    Parameters:\n        - star_info: star metadata DataFrame\n        - data: 3d data array (samples, time, frequencies)\n    \n    Returns:\n        tuple of DataFrame and outliers mask\n    \"\"\"\n    df = pd.DataFrame()\n\n    cut_inf, cut_sup = 36, 318\n    \n    signal = np.concatenate(\n        [data[:, :, 0][:, :, None], data[:, :, cut_inf:cut_sup]], axis=2\n    )\n    max_len = signal.shape[1] - 1\n        \n    lower, mid1, mid2, upper, lower1, upper1 = get_breakpoints(signal[:, :, 1:].mean(-1))\n    boundaries = (lower, mid1, mid2, upper, lower1, upper1) \n\n    # identifying outliers\n    outliers = (np.array(lower1) < bad_low) | (np.array(upper1) > bad_up) | (np.array(lower) < bad_low) | (np.array(upper) > bad_up)\n    for i in range(signal.shape[0]):\n        if not (lower[i] < mid1[i] < mid2[i] < upper[i]):\n            outliers[i] = 1\n    \n    signal_mean_raw = np.zeros(signal.shape[:2])\n    for i in tqdm(range(signal_mean_raw.shape[0])): # weighted averaging along frequency dimension\n        weights = calculate_weights(signal[i, :, 1:], *(b[i] for b in boundaries), outliers[i])\n        signal_mean_raw[i, :] = (signal[i, :, 1:] @ weights.T).T\n\n    signal_mean = savgol_filter(signal_mean_raw, 11, 1)\n\n    # frequency set for precise depth estimation via curve fitting (less robust)\n    good_waves = [1, 6, 11, 16, 21, 26, 31, 36, 41, 51, 61, 71, 76, 81, 86, 91, 96, 101, 106, 111, 121, 131, 141, 151, 161, 171, 196, 201, 206]\n    for i in tqdm(range(len(signal_mean))):\n\n        # filter very bad cases :) (exclude from training, use larger sigma for prediction)\n        df.loc[i, 'very_bad'] = (lower1[i] < bad_low // 2 or upper1[i] > max_len - (max_len - bad_up) // 2)\n        if os.environ[\"PREPROCESS_MODE\"] == 'train' and star_info.loc[i, 'planet_id'] in [2486733311, 2554492145]:\n            df.loc[i, 'very_bad'] = True\n\n\n        # averaged and mid-transit depth estimation\n        fake_avg, fake_mid = calc_depth_and_detrend(signal_mean[i].copy(), *(b[i] for b in boundaries), outliers[i], unstable=True)\n        fake_avg_2, fake_mid_2 = calc_depth_and_detrend(signal_mean[i].copy(), *(b[i] for b in boundaries), outliers[i], fixed_degree=3)\n        df.loc[i, 'average_depth'], df.loc[i, 'mid_depth'] = calc_depth_and_detrend(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n\n        norm_coef = (1 - df.loc[i, 'average_depth']) / (1 - fake_avg)\n        norm_coef_mid = (1 - df.loc[i, 'mid_depth']) / (1 - fake_mid)\n        norm_coef_2 = (1 - df.loc[i, 'average_depth']) / (1 - fake_avg_2)\n                        \n        fg1_signal = signal[i, :, 0].copy()\n        fg1_signal = savgol_filter(fg1_signal, 11, 1)\n        fg1_slope = savgol_filter(signal[i, :, 0].copy(), 41, 2)\n        _, _ = calc_depth_and_detrend(fg1_slope, *(b[i] for b in boundaries), outliers[i], fixed_degree=3) # for detrending\n        \n        df.loc[i, 'fg1_average_depth'], df.loc[i, 'fg1_mid_depth'] = calc_depth_and_detrend(fg1_signal, *(b[i] for b in boundaries), \n                                                                                            outliers[i], \n                                                                                            fixed_degree=3)\n\n        \n        # transit depth percentiles\n        for q in [0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n            df.loc[i, f'q_1_{q}'] = calc_perc(signal_mean[i], mid1[i], mid2[i], q, outliers[i])\n            df.loc[i, f'q_2_{q}'] = calc_perc(signal_mean[i], lower[i] - buf1, upper[i] + buf1, q, outliers[i])\n            df.loc[i, f'q_3_{q}'] = calc_perc(signal_mean[i], lower[i], upper[i], q, outliers[i]) \n        for q in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.8, 0.9]:\n            df.loc[i, f'fg1_q_1_{q}'] = calc_perc(fg1_signal, mid1[i], mid2[i], q, outliers[i])\n            df.loc[i, f'fg1_q_2_{q}'] = calc_perc(fg1_signal, lower[i], upper[i], q, outliers[i])\n            df.loc[i, f'fg1_q_3_{q}'] = calc_perc(fg1_signal, lower[i] - buf1, upper[i] + buf1, q, outliers[i])\n\n        \n        # slope features\n        df.loc[i, 'slope'] = calc_slope(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_2'] = calc_slope_2(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_2_left'] = calc_slope_2_left(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_2_right'] = calc_slope_2_right(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_g'] = max(0, -df.loc[i, 'slope_2'])**0.5\n                          \n        df.loc[i, 'fg1_slope'] = calc_slope(fg1_slope, *(b[i] for b in boundaries), outliers[i])     \n        df.loc[i, 'fg1_slope_2'] = calc_slope_2(fg1_slope, *(b[i] for b in boundaries), outliers[i])      \n        df.loc[i, 'fg1_slope_g'] = max(0, -df.loc[i, 'fg1_slope_2'])**0.5\n        df.loc[i, 'fg1_curv_left'] = calc_curv_left(fg1_slope, *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'fg1_curv_right'] = calc_curv_right(fg1_slope, *(b[i] for b in boundaries), outliers[i])\n\n        \n        # combinations with slopes\n        df.loc[i, 'slope_rel'] = df.loc[i, 'slope_2'] * df.loc[i, 'average_depth']\n        df.loc[i, 'fg1_slope_T'] = df.loc[i, 'fg1_slope_2'] * star_info.loc[i, 'Ts'] \n        df.loc[i, 'fg1_slope_rel'] = df.loc[i, 'fg1_slope_2'] * df.loc[i, 'fg1_average_depth']\n        df.loc[i, 'fg1_slope_g_rel'] = df.loc[i, 'fg1_slope_g'] * df.loc[i, 'fg1_average_depth']\n        \n        for q in [0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n            df.loc[i, f'slope_q_{q}'] = df.loc[i, 'slope_2'] * df.loc[i, f'q_1_{q}']\n            df.loc[i, f'slope_q_{q}_2'] = df.loc[i, 'slope_2'] * df.loc[i, f'q_2_{q}']\n\n        \n        # other features\n        df.loc[i, 't14'] = upper[i] - lower[i]\n        df.loc[i, 't23'] = mid2[i] - mid1[i]\n        df.loc[i, 'time'] = (mid1[i] - lower[i]) / (upper[i] - lower[i])        \n        df.loc[i, 'P_mul_Rs'] = star_info.loc[i, 'P'] * star_info.loc[i, 'Rs']\n        df.loc[i, 'P_div_Rs'] = star_info.loc[i, 'P'] / star_info.loc[i, 'Rs']\n        \n        step = 5\n        max_rel = 0\n        min_rel = 1\n        meaning = 60 # window size for frequency averagning\n        \n        for j in range(1, signal.shape[-1] - meaning + 1, step):\n            if j <= 80:\n                meaning = 20\n            elif j <= 180:\n                meaning = 30\n            else:\n                meaning = 60\n\n            cur_mean = signal[i, :, j : min(j + meaning, signal.shape[-1])].mean(-1)\n\n            # median filter\n            if not outliers[i]:\n                if j >= 180:\n                    med_kernel = 31\n                else:\n                    med_kernel = 21\n                cur_mean = median_filter(cur_mean, size=med_kernel, mode=\"constant\")\n                           \n            cur_mean = savgol_filter(cur_mean, 11, 1)\n            \n            df.loc[i, f'averaged_{j}_unstable'], df.loc[i, f'mid_{j}_unstable'] = calc_depth_and_detrend(cur_mean.copy(), *(b[i] for b in boundaries), \n                                                                                                            outliers[i],\n                                                                                                            unstable=True)  \n            if not outliers[i]:\n                df.loc[i, f'averaged_{j}_unstable'] = 1 - (1 - df.loc[i, f'averaged_{j}_unstable']) * norm_coef\n  \n            if j in good_waves:\n                df.loc[i, f'averaged_{j}'], df.loc[i, f'mid_{j}'] = calc_depth_and_detrend(cur_mean, *(b[i] for b in boundaries),\n                                                                                              outliers[i],\n                                                                                              fixed_degree=3)\n                if not outliers[i]:\n                    df.loc[i, f'averaged_{j}'] = 1 - (1 - df.loc[i, f'averaged_{j}']) * norm_coef_2\n\n            \n            # percentiles\n            for q in [0.1, 0.15, 0.2]:\n                if j in good_waves and not outliers[i]:\n                    df.loc[i, f'q_w_{j}_{q}'] = calc_perc(cur_mean, mid1[i], mid2[i], q, outliers[i])\n                elif outliers[i] and mid1[i] < mid2[i]:\n                    x_combined = np.concatenate([np.arange(max(lower[i] - buf1, 1)), np.arange(min(upper[i] + buf1, max_len), max_len + 1)], axis=-1)\n                    mid_q = np.quantile(cur_mean[mid1[i] : mid2[i]], q)\n                    df.loc[i, f'q_w_{j}_{q}'] = 1 - mid_q / cur_mean[x_combined].mean()\n                else:\n                    df.loc[i, f'q_w_{j}_{q}'] = 0\n           \n            \n            max_rel = max(max_rel, df.loc[i, f'averaged_{j}_unstable'])\n            min_rel = min(min_rel, df.loc[i, f'averaged_{j}_unstable'])\n\n  \n            # slope combinations\n            df.loc[i, f'averaged_slope_{j}'] = df.loc[i, f'averaged_{j}_unstable'] * df.loc[i, 'slope_2']\n            df.loc[i, f'averaged_slope_g_{j}'] = df.loc[i, f'averaged_{j}_unstable'] * df.loc[i, 'slope_g']\n\n        \n        # large amplitude     \n        if max_rel - min_rel >= 0.005:\n            df.loc[i, 'very_bad'] = True\n\n    \n    df['Rs'] = star_info['Rs']\n    df['Ms'] = star_info['Ms']\n    df['Ts'] = star_info['Ts']\n    df['sma'] = star_info['sma']  \n    df['g'] = np.log10(star_info['Ms'] / (star_info['Rs']**2))\n    df['g_T'] = df['g'] * star_info['Ts']\n    df['big_rs'] = (star_info['Rs'] > np.quantile(star_info['Rs'].values, 0.97))\n    \n    df['outliers'] = outliers\n\n    df = df.fillna(0)\n    \n    return outliers, df","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T17:41:02.263114Z","iopub.execute_input":"2025-11-14T17:41:02.263626Z","iopub.status.idle":"2025-11-14T17:41:02.274647Z","shell.execute_reply.started":"2025-11-14T17:41:02.263602Z","shell.execute_reply":"2025-11-14T17:41:02.273896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exec(open('feature_engineering.py', 'r').read())\noutliers, train = feature_engineering(train_star_info, data_train)\noutliers = np.arange(train.shape[0])[outliers]\nlen(outliers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:41:43.972909Z","iopub.execute_input":"2025-11-14T17:41:43.973189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# feature engineering file 2\n# %%writefile feature_engineering.py\n\nfrom scipy.signal import savgol_filter\nimport warnings\nfrom scipy.stats import kurtosis, skew\nfrom scipy.signal import medfilt\nfrom scipy.optimize import curve_fit\nfrom scipy.interpolate import interp1d\nfrom numpy.polynomial import Polynomial\nfrom scipy.optimize import least_squares, minimize\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.ndimage import median_filter\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\n\n\nwarnings.simplefilter(\"ignore\")\n\nA_BINNING = 15\n\n# threshold values for identifying outliers\nbad_low = 20\nbad_up = 354\n\nbuf = 15  # for lower1, upper1\nbuf1 = 20  # for lower, mid1, mid2, upper\n\n\nclass FeatureEngineeringConfig:\n    \"\"\"\n    Configuration class to store dataset-level statistics\n    calculated from training data only\n    \"\"\"\n    def __init__(self):\n        self.rs_threshold_97 = None\n    \n    def fit(self, train_star_info):\n        \"\"\"Calculate statistics from training data\"\"\"\n        self.rs_threshold_97 = np.quantile(train_star_info['Rs'].values, 0.97)\n        return self\n    \n    def save(self, filepath):\n        \"\"\"Save config for later use\"\"\"\n        import pickle\n        with open(filepath, 'wb') as f:\n            pickle.dump(self, f)\n    \n    @staticmethod\n    def load(filepath):\n        \"\"\"Load saved config\"\"\"\n        import pickle\n        with open(filepath, 'rb') as f:\n            return pickle.load(f)\n\n\ndef calculate_weights(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    \"\"\"\n    Calculating weights for averaging based on SNR\n    \"\"\"\n    max_len = a.shape[0] - 1\n    if is_outlier:\n        return np.ones_like(a[0]) / a.shape[-1]\n    else:\n        y_combined = np.concatenate([a[:max(lower - buf1, 1), :], a[min(upper + buf1, max_len):, :]], axis=0)\n        ratio = y_combined.mean(0) / y_combined.std(0)\n        return ratio / ratio.sum()\n\n\ndef calc_for_outliers(a, lower, upper):\n    \"\"\"\n    Estimating transit depth for outlier cases\n    \"\"\"\n    max_len = len(a) - 1\n            \n    if lower + buf < upper - buf:\n        obs = a[lower + buf : upper - buf].mean()\n    else:\n        obs = a[lower : upper].mean()\n\n    if lower - buf >= 10 and max_len - upper - buf >= 10:\n        unobs = (np.median(a[:max(lower - buf, 1)]) + np.median(a[min(upper + buf, max_len):])) / 2\n    elif lower >= max_len - upper:\n        unobs = np.median(a[:max(lower - buf, 1)])\n    else:\n        unobs = np.median(a[min(upper + buf, max_len):])\n\n    arr1 = 1 - (obs / unobs)\n    arr2 = 1 - a[(lower + upper) // 2] / unobs\n\n    return arr1, arr2\n\n\ndef calc_err(x_combined, y_combined, degree):\n    \"\"\"\n    Calculating the error to obtain the optimal polynomial degree\n    \"\"\"\n    max_len = 374 # hardcoded for BINNING=15\n    \n    poly_guess = np.polyfit(x_combined, y_combined, degree)\n    inter = np.polyval(poly_guess, np.arange(max_len + 1))\n    err = mean_squared_error(y_combined, inter[x_combined], squared=False)\n\n    # penalizing rmse, high polynomial degree and small number of points in curve fitting.\n    return err * degree**(1 - len(x_combined) / max_len)\n\n    \ndef calc_depth_and_detrend(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False, unstable=False, fixed_degree=2):\n    \"\"\"\n    Main function for transit depth estimation and detrending\n    \n    Parameters:\n        - a: 1d numpy array of observation points\n        - lower, mid1, mid2, upper, lower1, upper1: transit boundary points\n        - is_outlier: boolean flag indicating if the data point is an outlier\n        - unstable: if True, don't use curve fitting\n        - fixed_degree: if not None, use provided degree; otherwise, find optimal degree\n    \n    Returns:\n        - (arr1, arr2): tuple containing the averaged transit depth and the transit depth at mid-transit\n    \"\"\"\n    max_len = len(a) - 1\n    degree = 2\n    \n    if is_outlier:\n        a /= a.mean()\n        arr1, arr2 = calc_for_outliers(a, lower1, upper1)\n    else:\n        a /= a.mean()\n\n        # region outside the transit\n        x_combined = np.concatenate([np.arange(max(lower - buf1, 1)), np.arange(min(upper + buf1, max_len), max_len + 1)], axis=-1)\n        y_combined = a[x_combined]\n            \n        if fixed_degree is None: # find the optimal degree\n            best_val = 10**100\n            for j in [1, 2, 3, 4, 5]:\n                if calc_err(x_combined, y_combined, j) < best_val:\n                    best_val = calc_err(x_combined, y_combined, j)\n                    degree = j\n        else:\n            degree = fixed_degree\n\n        obs = a[mid1 : mid2]\n        \n        if unstable: # no curve fitting\n            unobs = y_combined.mean()\n        else:\n            poly_guess = np.polyfit(x_combined, y_combined, degree)         \n            inter = np.polyval(poly_guess, np.arange(max_len + 1))\n                \n            a /= inter\n            inter /= inter\n            unobs = inter[mid1 : mid2]\n\n        arr1 = 1 - np.mean(obs / unobs)\n        arr2 = 1 - a[(lower1 + upper1) // 2] / unobs.mean()\n                \n\n    if np.isnan(arr1):\n        arr1 = 0\n    if np.isnan(arr2):\n        arr2 = 0\n        \n    return arr1, arr2\n\n\ndef calc_slope(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    \"\"\"\n    Calculate transit wall steepness as slope between contact points\n    \"\"\"\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        return ((a[mid1] - a[lower]) / (mid1 - lower) - (a[upper] - a[mid2]) / (upper - mid2)) / 2\n\n\ndef calc_slope_2(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    \"\"\"\n    Calculate transit bottom curvature as slope between mid-transit \n    and contact point\n    \"\"\"\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        if not (mid1 < mid_ind < mid2):\n            return 0\n        return ((a[mid_ind] - a[mid1]) / (mid_ind - mid1) - (a[mid2] - a[mid_ind]) / (mid2 - mid_ind)) / 2\n\n\n# calculating slopes for asymmetric cases\ndef calc_slope_2_left(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper):\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        if not (mid1 < mid_ind < mid2):\n            return 0\n        return (a[mid_ind] - a[mid1]) / (mid_ind - mid1)\n\n\ndef calc_slope_2_right(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    max_len = len(a) - 1\n    \n    if not (lower < mid1 < mid2 < upper):\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        if not (mid1 < mid_ind < mid2):\n            return 0\n        return (a[mid2] - a[mid_ind]) / (mid2 - mid_ind)\n\n\n# gradient slopes\ndef calc_curv_left(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        a = savgol_filter(np.gradient(a), 41, 1)\n        return (a[mid_ind] - a[mid1]) / (mid_ind - mid1)\n\n\ndef calc_curv_right(a, lower, mid1, mid2, upper, lower1, upper1, is_outlier=False):\n    if not (lower < mid1 < mid2 < upper) or is_outlier:\n        return 0\n    else:\n        mid_ind = (mid1 + mid2) // 2\n        a = savgol_filter(np.gradient(a), 41, 1)\n        return (a[mid2] - a[mid_ind]) / (mid2 - mid_ind)\n\n\ndef calc_perc(a, lower, upper, q, is_outlier=False):\n    \"\"\"\n    Calculate the percentile of the transit depth, assumes the input signal is already detrended\n    \"\"\"\n    if is_outlier:\n        return 0\n    return np.quantile(1 - a[lower : upper], q)\n    \n\ndef feature_engineering(star_info, data, config=None, is_training=False):\n    \"\"\"\n    Prepares features for training or inference\n    \n    Parameters:\n        - star_info: star metadata DataFrame\n        - data: 3d data array (samples, time, frequencies)\n        - config: FeatureEngineeringConfig object (required if not training)\n        - is_training: whether this is training data (will fit config if True)\n    \n    Returns:\n        tuple of (outliers mask, DataFrame, config)\n    \"\"\"\n    df = pd.DataFrame()\n\n    # Initialize or validate config\n    if is_training:\n        if config is None:\n            config = FeatureEngineeringConfig()\n        config.fit(star_info)\n    else:\n        if config is None:\n            raise ValueError(\"config must be provided when is_training=False\")\n\n    cut_inf, cut_sup = 36, 318\n    \n    signal = np.concatenate(\n        [data[:, :, 0][:, :, None], data[:, :, cut_inf:cut_sup]], axis=2\n    )\n    max_len = signal.shape[1] - 1\n        \n    lower, mid1, mid2, upper, lower1, upper1 = get_breakpoints(signal[:, :, 1:].mean(-1))\n    boundaries = (lower, mid1, mid2, upper, lower1, upper1) \n\n    # identifying outliers\n    outliers = (np.array(lower1) < bad_low) | (np.array(upper1) > bad_up) | (np.array(lower) < bad_low) | (np.array(upper) > bad_up)\n    for i in range(signal.shape[0]):\n        if not (lower[i] < mid1[i] < mid2[i] < upper[i]):\n            outliers[i] = 1\n    \n    signal_mean_raw = np.zeros(signal.shape[:2])\n    for i in tqdm(range(signal_mean_raw.shape[0])): # weighted averaging along frequency dimension\n        weights = calculate_weights(signal[i, :, 1:], *(b[i] for b in boundaries), outliers[i])\n        signal_mean_raw[i, :] = (signal[i, :, 1:] @ weights.T).T\n\n    signal_mean = savgol_filter(signal_mean_raw, 11, 1)\n\n    # frequency set for precise depth estimation via curve fitting (less robust)\n    good_waves = [1, 6, 11, 16, 21, 26, 31, 36, 41, 51, 61, 71, 76, 81, 86, 91, 96, 101, 106, 111, 121, 131, 141, 151, 161, 171, 196, 201, 206]\n    \n    for i in tqdm(range(len(signal_mean))):\n\n        # Filter very bad cases based on data characteristics only\n        df.loc[i, 'very_bad'] = (lower1[i] < bad_low // 2 or upper1[i] > max_len - (max_len - bad_up) // 2)\n\n        # averaged and mid-transit depth estimation\n        fake_avg, fake_mid = calc_depth_and_detrend(signal_mean[i].copy(), *(b[i] for b in boundaries), outliers[i], unstable=True)\n        fake_avg_2, fake_mid_2 = calc_depth_and_detrend(signal_mean[i].copy(), *(b[i] for b in boundaries), outliers[i], fixed_degree=3)\n        df.loc[i, 'average_depth'], df.loc[i, 'mid_depth'] = calc_depth_and_detrend(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n\n        norm_coef = (1 - df.loc[i, 'average_depth']) / (1 - fake_avg)\n        norm_coef_mid = (1 - df.loc[i, 'mid_depth']) / (1 - fake_mid)\n        norm_coef_2 = (1 - df.loc[i, 'average_depth']) / (1 - fake_avg_2)\n                        \n        fg1_signal = signal[i, :, 0].copy()\n        fg1_signal = savgol_filter(fg1_signal, 11, 1)\n        fg1_slope = savgol_filter(signal[i, :, 0].copy(), 41, 2)\n        _, _ = calc_depth_and_detrend(fg1_slope, *(b[i] for b in boundaries), outliers[i], fixed_degree=3) # for detrending\n        \n        df.loc[i, 'fg1_average_depth'], df.loc[i, 'fg1_mid_depth'] = calc_depth_and_detrend(fg1_signal, *(b[i] for b in boundaries), \n                                                                                            outliers[i], \n                                                                                            fixed_degree=3)\n\n        \n        # transit depth percentiles\n        for q in [0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n            df.loc[i, f'q_1_{q}'] = calc_perc(signal_mean[i], mid1[i], mid2[i], q, outliers[i])\n            df.loc[i, f'q_2_{q}'] = calc_perc(signal_mean[i], lower[i] - buf1, upper[i] + buf1, q, outliers[i])\n            df.loc[i, f'q_3_{q}'] = calc_perc(signal_mean[i], lower[i], upper[i], q, outliers[i]) \n        for q in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.8, 0.9]:\n            df.loc[i, f'fg1_q_1_{q}'] = calc_perc(fg1_signal, mid1[i], mid2[i], q, outliers[i])\n            df.loc[i, f'fg1_q_2_{q}'] = calc_perc(fg1_signal, lower[i], upper[i], q, outliers[i])\n            df.loc[i, f'fg1_q_3_{q}'] = calc_perc(fg1_signal, lower[i] - buf1, upper[i] + buf1, q, outliers[i])\n\n        \n        # slope features\n        df.loc[i, 'slope'] = calc_slope(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_2'] = calc_slope_2(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_2_left'] = calc_slope_2_left(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_2_right'] = calc_slope_2_right(signal_mean[i], *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'slope_g'] = max(0, -df.loc[i, 'slope_2'])**0.5\n                          \n        df.loc[i, 'fg1_slope'] = calc_slope(fg1_slope, *(b[i] for b in boundaries), outliers[i])     \n        df.loc[i, 'fg1_slope_2'] = calc_slope_2(fg1_slope, *(b[i] for b in boundaries), outliers[i])      \n        df.loc[i, 'fg1_slope_g'] = max(0, -df.loc[i, 'fg1_slope_2'])**0.5\n        df.loc[i, 'fg1_curv_left'] = calc_curv_left(fg1_slope, *(b[i] for b in boundaries), outliers[i])\n        df.loc[i, 'fg1_curv_right'] = calc_curv_right(fg1_slope, *(b[i] for b in boundaries), outliers[i])\n\n        \n        # combinations with slopes\n        df.loc[i, 'slope_rel'] = df.loc[i, 'slope_2'] * df.loc[i, 'average_depth']\n        df.loc[i, 'fg1_slope_T'] = df.loc[i, 'fg1_slope_2'] * star_info.loc[i, 'Ts'] \n        df.loc[i, 'fg1_slope_rel'] = df.loc[i, 'fg1_slope_2'] * df.loc[i, 'fg1_average_depth']\n        df.loc[i, 'fg1_slope_g_rel'] = df.loc[i, 'fg1_slope_g'] * df.loc[i, 'fg1_average_depth']\n        \n        for q in [0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n            df.loc[i, f'slope_q_{q}'] = df.loc[i, 'slope_2'] * df.loc[i, f'q_1_{q}']\n            df.loc[i, f'slope_q_{q}_2'] = df.loc[i, 'slope_2'] * df.loc[i, f'q_2_{q}']\n\n        \n        # other features\n        df.loc[i, 't14'] = upper[i] - lower[i]\n        df.loc[i, 't23'] = mid2[i] - mid1[i]\n        df.loc[i, 'time'] = (mid1[i] - lower[i]) / (upper[i] - lower[i])        \n        df.loc[i, 'P_mul_Rs'] = star_info.loc[i, 'P'] * star_info.loc[i, 'Rs']\n        df.loc[i, 'P_div_Rs'] = star_info.loc[i, 'P'] / star_info.loc[i, 'Rs']\n        \n        step = 5\n        max_rel = 0\n        min_rel = 1\n        meaning = 60 # window size for frequency averaging\n        \n        for j in range(1, signal.shape[-1] - meaning + 1, step):\n            if j <= 80:\n                meaning = 20\n            elif j <= 180:\n                meaning = 30\n            else:\n                meaning = 60\n\n            cur_mean = signal[i, :, j : min(j + meaning, signal.shape[-1])].mean(-1)\n\n            # median filter\n            if not outliers[i]:\n                if j >= 180:\n                    med_kernel = 31\n                else:\n                    med_kernel = 21\n                cur_mean = median_filter(cur_mean, size=med_kernel, mode=\"constant\")\n                           \n            cur_mean = savgol_filter(cur_mean, 11, 1)\n            \n            df.loc[i, f'averaged_{j}_unstable'], df.loc[i, f'mid_{j}_unstable'] = calc_depth_and_detrend(cur_mean.copy(), *(b[i] for b in boundaries), \n                                                                                                            outliers[i],\n                                                                                                            unstable=True)  \n            if not outliers[i]:\n                df.loc[i, f'averaged_{j}_unstable'] = 1 - (1 - df.loc[i, f'averaged_{j}_unstable']) * norm_coef\n  \n            if j in good_waves:\n                df.loc[i, f'averaged_{j}'], df.loc[i, f'mid_{j}'] = calc_depth_and_detrend(cur_mean, *(b[i] for b in boundaries),\n                                                                                              outliers[i],\n                                                                                              fixed_degree=3)\n                if not outliers[i]:\n                    df.loc[i, f'averaged_{j}'] = 1 - (1 - df.loc[i, f'averaged_{j}']) * norm_coef_2\n\n            \n            # percentiles\n            for q in [0.1, 0.15, 0.2]:\n                if j in good_waves and not outliers[i]:\n                    df.loc[i, f'q_w_{j}_{q}'] = calc_perc(cur_mean, mid1[i], mid2[i], q, outliers[i])\n                elif outliers[i] and mid1[i] < mid2[i]:\n                    x_combined = np.concatenate([np.arange(max(lower[i] - buf1, 1)), np.arange(min(upper[i] + buf1, max_len), max_len + 1)], axis=-1)\n                    mid_q = np.quantile(cur_mean[mid1[i] : mid2[i]], q)\n                    df.loc[i, f'q_w_{j}_{q}'] = 1 - mid_q / cur_mean[x_combined].mean()\n                else:\n                    df.loc[i, f'q_w_{j}_{q}'] = 0\n           \n            \n            max_rel = max(max_rel, df.loc[i, f'averaged_{j}_unstable'])\n            min_rel = min(min_rel, df.loc[i, f'averaged_{j}_unstable'])\n\n  \n            # slope combinations\n            df.loc[i, f'averaged_slope_{j}'] = df.loc[i, f'averaged_{j}_unstable'] * df.loc[i, 'slope_2']\n            df.loc[i, f'averaged_slope_g_{j}'] = df.loc[i, f'averaged_{j}_unstable'] * df.loc[i, 'slope_g']\n\n        \n        # large amplitude     \n        if max_rel - min_rel >= 0.005:\n            df.loc[i, 'very_bad'] = True\n\n    \n    df['Rs'] = star_info['Rs']\n    df['Ms'] = star_info['Ms']\n    df['Ts'] = star_info['Ts']\n    df['sma'] = star_info['sma']  \n    df['g'] = np.log10(star_info['Ms'] / (star_info['Rs']**2))\n    df['g_T'] = df['g'] * star_info['Ts']\n    \n    # Use pre-calculated threshold from training data\n    df['big_rs'] = (star_info['Rs'] > config.rs_threshold_97)\n    \n    df['outliers'] = outliers\n\n    df = df.fillna(0)\n    \n    return outliers, df, config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:57:55.767050Z","iopub.execute_input":"2025-11-14T17:57:55.767372Z","iopub.status.idle":"2025-11-14T17:57:55.815653Z","shell.execute_reply.started":"2025-11-14T17:57:55.767348Z","shell.execute_reply":"2025-11-14T17:57:55.815056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\noutliers_train, df_train, config = feature_engineering(\n    train_star_info, \n    data_train, \n    is_training=True\n)\n\n# Save config for later\nconfig.save('feature_config.pkl')\n\n# # Inference/Testing\n# config = FeatureEngineeringConfig.load('feature_config.pkl')\n# outliers_test, df_test, _ = feature_engineering(\n#     test_star_info, \n#     test_data, \n#     config=config,\n#     is_training=False\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T17:58:59.795269Z","iopub.execute_input":"2025-11-14T17:58:59.795573Z","iopub.status.idle":"2025-11-14T18:09:56.543152Z","shell.execute_reply.started":"2025-11-14T17:58:59.795554Z","shell.execute_reply":"2025-11-14T18:09:56.542285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference/Testing\n\n# test_star_info = f\"{DATA_ROOT}/test_star_info.csv\"\n# config = FeatureEngineeringConfig.load('feature_config.pkl')\n# outliers_test, df_test, _ = feature_engineering(\n#     test_star_info, \n#     test_data, \n#     config=config,\n#     is_training=False\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:12:42.012749Z","iopub.execute_input":"2025-11-14T18:12:42.013485Z","iopub.status.idle":"2025-11-14T18:12:42.024556Z","shell.execute_reply.started":"2025-11-14T18:12:42.013459Z","shell.execute_reply":"2025-11-14T18:12:42.023530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nfrom sklearn.utils.validation import check_is_fitted\n\n\nclass CustomRidge(BaseEstimator):\n    \"\"\"\n    Provides two models: main model for normal cases and outlier-specific model\n    \"\"\"\n    def __init__(self):\n        self.main = Ridge(alpha=3e-2) \n        self.outliers = Ridge(alpha=3e-1)\n\n        self.main_scaler = RobustScaler()\n        self.outliers_scaler = RobustScaler()\n\n\n    def fit(self, X, y):\n        groups = X['outliers']\n        X = X.drop(columns=['outliers'])\n\n        main_mask = (groups == 0).values\n        main_mask[X.loc[:, 'very_bad'] == True] = 0\n        \n        X_main = self.main_scaler.fit_transform(X[main_mask])\n\n        self.main.fit(self.main_scaler.transform(X[main_mask]), y[main_mask])\n        self.outliers.fit(self.outliers_scaler.fit_transform(X), y)     \n        self.pred_shape = y.shape[-1]\n\n        return self\n\n    def predict(self, X):\n        groups = X['outliers']\n        X = X.drop(columns=['outliers'])\n        \n        predictions = np.zeros((X.shape[0], self.pred_shape))\n\n        main_mask = (groups == 0).values\n        if main_mask.sum():\n            predictions[main_mask] = self.main.predict(self.main_scaler.transform(X[main_mask]))                                                 \n        if (~main_mask).sum():\n            predictions[~main_mask] = self.outliers.predict(self.outliers_scaler.transform(X[~main_mask]))\n        \n        return predictions\n\n\nmodel = CustomRidge()\n\noof_pred = cross_val_predict(model, train, train_labels.values, cv=100)\n\nprint(f\"# R2 score: {r2_score(train_labels, oof_pred):.3f}\")\n\nsigma_pred = mean_squared_error(train_labels, oof_pred, squared=False)\n  \nprint(f\"# Root mean squared error: {sigma_pred:.6f}\")\n\ncol = 1\nplt.scatter(oof_pred[:,col], train_labels.iloc[:,col], s=15, c='lightgreen')\nplt.gca().set_aspect('equal')\nplt.xlabel('y_pred')\nplt.ylabel('y_true')\nplt.title('Comparing y_true and y_pred')\nplt.show()\n\n# clipping\noof_pred = np.maximum(oof_pred, 0.003)\noof_pred = np.minimum(oof_pred, 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:21:09.233930Z","iopub.execute_input":"2025-11-14T18:21:09.234216Z","iopub.status.idle":"2025-11-14T18:22:02.839348Z","shell.execute_reply.started":"2025-11-14T18:21:09.234194Z","shell.execute_reply":"2025-11-14T18:22:02.838818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Split manually before CV\n# from sklearn.model_selection import train_test_split\n\n# X_train, X_valid, y_train, y_valid = train_test_split(\n#     train, train_labels, test_size=0.2, random_state=42\n# )\n\n# model.fit(X_train, y_train)\n# y_pred = model.predict(X_valid)\n\n# print(\"R2:\", r2_score(y_valid, y_pred))\n# print(\"RMSE:\", mean_squared_error(y_valid, y_pred, squared=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import KFold, cross_val_score\n\n# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n# scores = cross_val_score(model, train, train_labels.values, cv=cv, scoring='r2')\n# print(\"Mean R¬≤:\", np.mean(scores), \"¬±\", np.std(scores))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(train.shape, train_labels.shape)\n# print(\"Same index:\", (train.index == train_labels.index).all())\n# train, train_labels = train.align(train_labels, axis=0, join='inner')\n# print(\"Same index:\", (train.index == train_labels.index).all())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.utils import shuffle\n\n# y_shuffled = shuffle(train_labels)\n# shuffled_score = np.mean(cross_val_score(model, train, y_shuffled, cv=5, scoring='r2'))\n# print(\"Shuffled-label R¬≤:\", shuffled_score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import learning_curve\n\n# train_sizes, train_scores, test_scores = learning_curve(\n#     model, train, train_labels.values, cv=5, scoring='r2'\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(train_scores, test_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SigmaPredictor:\n    \"\"\"\n    Class for sigma predicting\n    \"\"\"\n    def __init__(self):\n        self.sigmas = {}\n        \n    def fit(self, y_pred, y_true, outliers, very_bad):        \n        outliers = [i for i in outliers if i not in very_bad]\n\n        self.sigmas['outliers'] = self._calc(y_pred[outliers], y_true[outliers]) * 5\n\n        main = self._del_outliers(np.ones(len(y_pred), dtype=bool), outliers + list(very_bad))\n        self.sigmas['main'] = self._calc(y_pred[main], y_true[main])\n\n        print({ k: v.mean() for k, v in self.sigmas.items() })\n\n    def predict(self, sigma_pred, y_pred, outliers, very_bad, bootstrap_preds=None):\n        if len(outliers) > 0:\n            sigma_pred[outliers] = self.sigmas['outliers']\n\n        main = self._del_outliers(np.ones(len(y_pred), dtype=bool), outliers)\n        if main.sum() > 0:\n            sigma_pred[main] = self.sigmas['main']\n\n        W1 = 0.75\n        W2 = 1.0 - W1\n        \n        sigma_pred[main, :] = bootstrap_preds[main, :] * W1 + sigma_pred[main, :] * W2\n        sigma_pred[outliers] = bootstrap_preds[outliers] * 1.5\n\n        sigma_pred[very_bad] = 0.003 \n        for i in very_bad:\n            if i in outliers:\n                continue\n            sigma_pred[i, :] = 0.5 * bootstrap_preds[i, :] + 0.5 * sigma_pred[i, :]\n\n        return sigma_pred\n        \n\n    def _calc(self, y_pred, y_true): # calculate rmse for each frequency\n        sigmas = []\n        for i in range(y_pred.shape[1]):\n            sigmas.append(mean_squared_error(y_pred[:, i], y_true[:, i], squared=False))\n        return np.array(sigmas)\n\n    def _del_outliers(self, mask, outliers):\n        for i in range(len(mask)):\n            if i in outliers:\n                mask[i] = False\n        return mask                         \n\n\ndef postprocessing(pred_array, index, sigma_pred, sigma_predictor, outliers, very_bad, bootstrap_preds=None, column_names=None):\n    \"\"\"\n    Creates a submission DataFrame with mean predictions and uncertainties.\n\n    Parameters:\n    - pred_array: ndarray of shape (n_samples, 283)\n    - index: pandas.Index of length n_samples\n    - sigma_pred: float or ndarray of shape (n_samples, 283)\n    - column_names: list of wavelength column names (optional)\n\n    Returns:\n    - df: DataFrame of shape (n_samples, 566)\n    \"\"\"\n    n_samples, n_waves = pred_array.shape\n\n    if column_names is None:\n        column_names = [f\"wl_{i+1}\" for i in range(n_waves)]\n    \n    sigma_pred = sigma_predictor.predict(np.zeros_like(pred_array), pred_array, outliers, very_bad, bootstrap_preds=bootstrap_preds)\n\n    # Safety check\n    assert sigma_pred.shape == pred_array.shape, \"sigma_pred must match shape of pred_array\"\n    assert len(index) == n_samples, \"Index length must match number of rows\"\n\n    df_mean = pd.DataFrame(pred_array.clip(0, None), index=index, columns=column_names)\n    df_sigma = pd.DataFrame(sigma_pred, index=index, columns=[f\"sigma_{i+1}\" for i in range(n_waves)])\n\n    return pd.concat([df_mean, df_sigma], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:22:53.200557Z","iopub.execute_input":"2025-11-14T18:22:53.201126Z","iopub.status.idle":"2025-11-14T18:22:53.212107Z","shell.execute_reply.started":"2025-11-14T18:22:53.201099Z","shell.execute_reply":"2025-11-14T18:22:53.211443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(train, train_labels)\n\nsigma_predictor = SigmaPredictor()\nvery_bad = np.arange(train_labels.shape[0])[train['very_bad'].values == True]\nsigma_predictor.fit(oof_pred, train_labels.values, outliers, very_bad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:22:57.459115Z","iopub.execute_input":"2025-11-14T18:22:57.459845Z","iopub.status.idle":"2025-11-14T18:22:58.014244Z","shell.execute_reply.started":"2025-11-14T18:22:57.459819Z","shell.execute_reply":"2025-11-14T18:22:58.013204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bootstrap_uncertainty_inference( \n    X_train,\n    y_train,\n    X_test,\n    n_bootstraps = 100, \n    random_state = 42,\n):\n    \"\"\"\n    Sigma estimation via bootstrapping\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n    \n    y_train_values = y_train\n        \n    n_test_samples = X_test.shape[0]\n    n_targets = y_train_values.shape[1]\n    \n    predictions = np.full((n_test_samples, n_targets, n_bootstraps), np.nan)\n    \n    bootstrap_iter = range(n_bootstraps)\n    bootstrap_iter = tqdm(bootstrap_iter, desc=\"bootstrap interations\")\n    \n    for b in bootstrap_iter:\n        bootstrap_indices = np.random.choice(\n            len(X_train), size=len(X_train), replace=True\n        )\n        \n        X_bootstrap = X_train.iloc[bootstrap_indices].reset_index(drop=True)\n        y_bootstrap = y_train_values.iloc[bootstrap_indices].reset_index(drop=True)\n        \n        model_bootstrap = CustomRidge()\n        model_bootstrap.fit(X_bootstrap, y_bootstrap)\n        \n        y_pred = model_bootstrap.predict(X_test)   \n        predictions[:, :, b] = y_pred\n     \n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:29:38.287359Z","iopub.execute_input":"2025-11-14T18:29:38.287605Z","iopub.status.idle":"2025-11-14T18:29:38.293528Z","shell.execute_reply.started":"2025-11-14T18:29:38.287588Z","shell.execute_reply":"2025-11-14T18:29:38.292886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport gc\nimport pandas as pd\nimport numpy as np\n\n# Load test data\ntest_adc_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/test_star_info.csv', index_col='planet_id')\nsample_submission = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/sample_submission.csv', index_col='planet_id')\nwavelengths = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/wavelengths.csv')\ntest_star_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2025/test_star_info.csv')\n\n# Clean up training data from memory\n# del data_train\ngc.collect()\n\n# Run preprocessing\nos.environ[\"PREPROCESS_MODE\"] = \"test\"\n!python preprocess.py\n!rm -rf *AIRS-CH0_signal*\n\n# Load preprocessed test signal\ndata_test = np.load(\"signal_v2.npy\")\n\n# Load the configuration fitted on training data\nconfig = FeatureEngineeringConfig.load('feature_config.pkl')\n\n# Generate features for test data\n# ‚úÖ FIXED: Use test_star_info DataFrame, not a string path\noutliers, test_features, _ = feature_engineering(\n    test_star_info,  # ‚úÖ This is the DataFrame, don't overwrite it!\n    data_test, \n    config=config,\n    is_training=False\n)\n\n# Get indices of outliers and very bad samples\noutliers = np.arange(test_features.shape[0])[outliers]\nvery_bad = np.arange(test_features.shape[0])[test_features['very_bad'].values == True]\n\n# Generate predictions\ntest_pred = model.predict(test_features)\n\n# Bootstrap uncertainty estimation\nboot_pred = bootstrap_uncertainty_inference(train, train_labels, test_features, n_bootstraps=1000)\ntest_bootstrap_preds = boot_pred.std(-1) * 2.75\n\n# Clip predictions to valid physical range\ntest_pred = np.maximum(test_pred, 0.003)\ntest_pred = np.minimum(test_pred, 0.1)\n\nprint('sigma', sigma_pred)\n\ndef postprocessing(pred_array, index, sigma_pred, sigma_predictor, outliers, very_bad, bootstrap_preds=None, column_names=None):\n    \"\"\"\n    Convert predictions and uncertainty into final submission DataFrame\n    \"\"\"\n    sigma_array = sigma_predictor.predict(np.zeros_like(pred_array), pred_array, outliers, very_bad, bootstrap_preds=bootstrap_preds)\n    df_pred = pd.DataFrame(pred_array.clip(0, None), index=index, columns=column_names)\n    df_sigma = pd.DataFrame(sigma_array, index=index, columns=[f\"sigma_{i}\" for i in range(1, len(column_names)+1)])\n    return pd.concat([df_pred, df_sigma], axis=1)\n\n# Create submission\nsubmission_df = postprocessing(\n    pred_array=test_pred,\n    index=sample_submission.index,\n    sigma_pred=sigma_pred,\n    sigma_predictor=sigma_predictor,\n    column_names=wavelengths.columns,\n    bootstrap_preds=test_bootstrap_preds,\n    outliers=outliers,\n    very_bad=very_bad\n)\n\n# Save submission\nsubmission_df.to_csv('submission.csv')\n!head submission.csv","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-11-14T18:44:14.307301Z","shell.execute_reply.started":"2025-11-14T18:37:16.316411Z","shell.execute_reply":"2025-11-14T18:44:14.306416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\nimport os\n\n# after training\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\njoblib.dump(model, \"/kaggle/working/models/custom_ridge.pkl\")\n\nprint(\"‚úÖ Model saved at /kaggle/working/models/custom_ridge.pkl\")\n\n\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\njoblib.dump(sigma_predictor, \"/kaggle/working/models/SigmaPredictor.pkl\")\n\nprint(\"‚úÖ sigma_predictor saved at /kaggle/working/models/SigmaPredictor.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:18:38.326896Z","iopub.execute_input":"2025-11-14T19:18:38.327789Z","iopub.status.idle":"2025-11-14T19:18:38.340994Z","shell.execute_reply.started":"2025-11-14T19:18:38.327741Z","shell.execute_reply":"2025-11-14T19:18:38.340183Z"}},"outputs":[],"execution_count":null}]}